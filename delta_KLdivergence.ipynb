{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92357824-74b3-4bba-9ad4-6a93954ba2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8ec81-bd4f-4c5e-b176-dbadeefdfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_subjects=['sub1','sub2','sub3','sub4','sub5','sub6','sub7','sub8','sub9','sub10','sub11','sub12','sub13','sub14','sub15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7aa2d-6c2b-4d37-8973-0e8fcc10ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_grid_no_list={'sub1':[0,1,2,3],\n",
    "                    'sub2':[0,1,2,3,4,5],\n",
    "                    'sub3':[0,1,2,3,4],\n",
    "                    'sub4':[0,1,2,3,4],\n",
    "                    'sub5':[0,1,2,3],\n",
    "                    'sub6':[0,1,2,3,4],\n",
    "                    'sub7':[0,1,2],\n",
    "                    'sub8':[0,2,3],\n",
    "                    'sub9':[0,1,2,3],\n",
    "                    'sub10':[0,1],\n",
    "                    'sub11':[0,1,2],\n",
    "                    'sub12':[0,1,2],\n",
    "                    'sub13':[0,1,2,3],\n",
    "                    'sub14':[0,1,2,3],\n",
    "                    'sub15':[0,1,2,3,4]\n",
    "}\n",
    "                   \n",
    "\n",
    "# Among the two lists in badnull_ans_dict, the first contains cases where the answer was stated incorrectly (i.e., the opposite of the correct answer), and the second contains experimental errors—such as failing to give an answer, speaking during the baseline period, or similar issues.\n",
    "liv_nonliv_badnull_ans_dict = {'sub1': [[], []],\n",
    "                               'sub2': [[], []],\n",
    "                               'sub3': [[49], []],\n",
    "                               'sub4': [[], []],\n",
    "                               'sub5': [[5, 7], []],\n",
    "                               'sub6': [[41], [17]],\n",
    "                               'sub7': [[], []],\n",
    "                               'sub8': [[], [33]],\n",
    "                               'sub9' : [[8],[13]],\n",
    "                               'sub10' : [[],[36]],\n",
    "                               'sub11' : [[],[]],\n",
    "                               'sub12' : [[],[]],\n",
    "                               'sub13' : [[16],[]],\n",
    "                               'sub14': [[10], [23]],\n",
    "                               'sub15': [[], []],\n",
    "                               }\n",
    "\n",
    "\n",
    "body_nonbody_badnull_ans_dict = {'sub1': [[], []],\n",
    "                               'sub2': [[], []],\n",
    "                               'sub3': [[], []],\n",
    "                               'sub4': [[], [3,7]],\n",
    "                               'sub5': [[], []],\n",
    "                               'sub6': [[], []],\n",
    "                               'sub7': [[], []],\n",
    "                               'sub8': [[], []],\n",
    "                               'sub9' : [[],[]],\n",
    "                               'sub10' : [[],[4]],\n",
    "                               'sub11' : [[],[]],\n",
    "                               'sub12' : [[],[]],\n",
    "                               'sub13' : [[],[]],\n",
    "                               'sub14': [[], [0]],\n",
    "                               'sub15': [[], []],\n",
    "                               }\n",
    "\n",
    "\n",
    "body_nonbody_badnull_ans_dict = {'sub1': [[], []],\n",
    "                                 'sub2': [[], []],\n",
    "                                 'sub3': [[], []],\n",
    "                                 'sub4': [[], [3+72, 7+72]],\n",
    "                                 'sub5': [[], []],\n",
    "                                 'sub6': [[], []],\n",
    "                                 'sub7': [[], []],\n",
    "                                 'sub8': [[], []],\n",
    "                                 'sub9' : [[],[]],\n",
    "                                 'sub10' : [[i for i in range(54,72)],[4+72]+[i for i in range(126,144)]],\n",
    "                                 'sub11' : [[],[]],\n",
    "                                 'sub12' : [[],[]],\n",
    "                                 'sub13' : [[],[]],\n",
    "                                 'sub14': [[], [0+72]],\n",
    "                                 'sub15': [[], []],\n",
    "                                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac34bf0-9bcb-46fe-8513-ed5cb12d3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 72 in bodypart\n",
    "def bad_trial_giver(pat_name, strip_grid_no_list):\n",
    "    total_badtrial=[[],[],[],[],[],[],[]]\n",
    "    ch_info_dirs = 'data_path/'+pat_name+'/'+pat_name+'good_ch.mat'\n",
    "    with h5py.File(ch_info_dirs, 'r') as file:\n",
    "        ch_info_raw = file['good_ch_index'][:]\n",
    "    max_grid_group = int(ch_info_raw[-1]//1000)\n",
    "    ch_info = [[] for i in range(max_grid_group)]\n",
    "    for each_ch_info in ch_info_raw:\n",
    "        grid_group = int(each_ch_info//1000)-1\n",
    "        grid_num = int(each_ch_info%1000)\n",
    "        ch_info[grid_group].append(grid_num)\n",
    "    ch_indexing=[0]\n",
    "    cnter_ch=0\n",
    "    strip_grid_no = str(strip_grid_no_list[pat_name])\n",
    "    file_path = \"l_\"+pat_name+'_'+strip_grid_no+\"_index.pkl\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        checkbox_indexs = pickle.load(f)\n",
    "    print(checkbox_indexs)\n",
    "    file_path = \"badtrials/l_\"+pat_name+'_'+strip_grid_no+\".pkl\"\n",
    "    with open(file_path, 'rb') as f: #6\n",
    "        l_badtrial_raw = np.array(pickle.load(f))\n",
    "    \n",
    "    file_path = \"badtrials/b_\"+pat_name+'_'+strip_grid_no+\".pkl\"\n",
    "    with open(file_path, 'rb') as f: #7\n",
    "        b_badtrial_raw = np.array(pickle.load(f))    \n",
    "    \n",
    "    for i in ch_info:\n",
    "        ch_indexing.append(ch_indexing[cnter_ch]+len(i))\n",
    "        cnter_ch +=1\n",
    "    \n",
    "    for idx in range(len(strip_grid_no_list[pat_name])):\n",
    "        checkbox_index = checkbox_indexs[idx]\n",
    "        ch_cnt = len(ch_info[strip_grid_no_list[pat_name][idx]])\n",
    "        for point in range(7):\n",
    "            check_bullean_l = np.where(np.sum(l_badtrial_raw[checkbox_index,:,:][:,[0,point+1]],axis=1)>0,1,0)\n",
    "            check_bullean_b = np.where(np.sum(b_badtrial_raw[checkbox_index,:,:][:,[0,point+1]],axis=1)>0,1,0)\n",
    "            bad_trial_list = []\n",
    "            for i in range(72):\n",
    "                if check_bullean_l[i] == 1:\n",
    "                    bad_trial_list.append(i)\n",
    "            for i in range(72):\n",
    "                if check_bullean_b[i] == 1:\n",
    "                    bad_trial_list.append(i+72)\n",
    "            for i in range(ch_cnt):\n",
    "                total_badtrial[point].append(bad_trial_list)\n",
    "    return total_badtrial\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a50e6c-801c-4ceb-91ce-79bbb1d7563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatgoodch(pat_name,strip_grid_no_list):\n",
    "    strip_grid_no = strip_grid_no_list[pat_name]\n",
    "    return_list=[]\n",
    "    ch_info_dirs = 'data_path/'+pat_name+'/'+pat_name+'good_ch.mat'\n",
    "    with h5py.File(ch_info_dirs, 'r') as file:\n",
    "        ch_info_raw = file['good_ch_index'][:]\n",
    "    max_grid_group = int(ch_info_raw[-1]//1000)\n",
    "    ch_info = [[] for i in range(max_grid_group)]\n",
    "    for each_ch_info in ch_info_raw:\n",
    "        grid_group = int(each_ch_info//1000)-1\n",
    "        grid_num = int(each_ch_info%1000)\n",
    "        ch_info[grid_group].append(grid_num)\n",
    "    ch_indexing=[0]\n",
    "    cnter_ch=0\n",
    "    for i in ch_info:\n",
    "        #print(len(i))\n",
    "        ch_indexing.append(ch_indexing[cnter_ch]+len(i))\n",
    "        cnter_ch +=1\n",
    "    for strip_grid_idx in strip_grid_no:\n",
    "        for i in range(ch_indexing[strip_grid_idx],ch_indexing[strip_grid_idx+1]):\n",
    "            return_list.append(i)\n",
    "    print(ch_indexing)\n",
    "    print(len(ch_info_raw))\n",
    "    print(len(return_list))\n",
    "    return return_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8356b7-713f-4dbd-a6a8-5cbfa69c300d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FILE LOAD\n",
    "\n",
    "q_offsettiming=[3407,4070,3823,4302] #offset time for each sentence\n",
    "\n",
    "def file_loader_and_npconcate_TFmean(pat_name,cutstart,cutend,ver):\n",
    "    dirs = 'D:/GwonYonghyeon/data/250102_preprocessed/'+pat_name+'/'+pat_name+'TF_freqenv_ver'+str(ver)\n",
    "    val_name = \"TF_mean_epoch\"\n",
    "    fs_ratio = 2\n",
    "    with h5py.File(dirs+'sem1.mat', 'r') as file:\n",
    "        data_sem1 = file[val_name][:]\n",
    "    data_sem1_copy = data_sem1[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem1\n",
    "    with h5py.File(dirs+'sem2.mat', 'r') as file:\n",
    "        data_sem2 = file[val_name][:]\n",
    "    data_sem2_copy = data_sem2[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem2\n",
    "    with h5py.File(dirs+'sem5.mat', 'r') as file:\n",
    "        data_sem5 = file[val_name][:]\n",
    "    data_sem5_copy = data_sem5[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem5\n",
    "    with h5py.File(dirs+'sem6.mat', 'r') as file:\n",
    "        data_sem6 = file[val_name][:]\n",
    "    data_sem6_copy = data_sem6[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem6\n",
    "\n",
    "    liv_nonliv_raw_data = np.concatenate((data_sem1_copy, data_sem2_copy,data_sem5_copy,data_sem6_copy), axis=0)\n",
    "    print(\"liv_nonliv_load_complete. || shape : \",   liv_nonliv_raw_data.shape)\n",
    "\n",
    "    with h5py.File(dirs+'sem3.mat', 'r') as file:\n",
    "        data_sem3 = file[val_name][:]\n",
    "    data_sem3_copy = data_sem3[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem3\n",
    "    with h5py.File(dirs+'sem4.mat', 'r') as file:\n",
    "        data_sem4 = file[val_name][:]\n",
    "    data_sem4_copy = data_sem4[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem4\n",
    "    with h5py.File(dirs+'sem7.mat', 'r') as file:\n",
    "        data_sem7 = file[val_name][:]\n",
    "    data_sem7_copy = data_sem7[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem7\n",
    "    with h5py.File(dirs+'sem8.mat', 'r') as file:\n",
    "        data_sem8 = file[val_name][:]\n",
    "    data_sem8_copy = data_sem8[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem8\n",
    "\n",
    "    body_nonbody_raw_data = np.concatenate((data_sem3_copy, data_sem4_copy,data_sem7_copy,data_sem8_copy), axis=0)\n",
    "    print(\"body_nonbody_load_complete. || shape : \",   body_nonbody_raw_data.shape)\n",
    "    \n",
    "    return liv_nonliv_raw_data, body_nonbody_raw_data\n",
    "\n",
    "def file_loader_and_npconcate_question_TFmean(pat_name,q_offset,liv_nonliv_raw_answer,body_nonbody_raw_answer,ver):\n",
    "    time_window = 600\n",
    "    dirs = 'D:/GwonYonghyeon/data/250102_preprocessed/'+pat_name+'/'+pat_name+'TF_freqenv_ver'+str(ver)\n",
    "    val_name =\"TF_mean_epoch\"\n",
    "    fs_ratio = 2 \n",
    "    with h5py.File(dirs+'sem1.mat', 'r') as file:\n",
    "        data_sem1 = file[val_name][:]\n",
    "    data_sem1_copy = np.zeros((data_sem1.shape[0],time_window*fs_ratio,data_sem1.shape[2],data_sem1.shape[3]))\n",
    "    for i in range(18):\n",
    "        if liv_nonliv_raw_answer[i] == 0:\n",
    "            data_sem1_copy[i,:,:,:] = data_sem1[i,fs_ratio*q_offset[0]:fs_ratio*(q_offset[0]+time_window),:,:].copy()\n",
    "        if liv_nonliv_raw_answer[i] == 1:\n",
    "            data_sem1_copy[i,:,:,:] = data_sem1[i,fs_ratio*q_offset[1]:fs_ratio*(q_offset[1]+time_window),:,:].copy()\n",
    "    del data_sem1\n",
    "    with h5py.File(dirs+'sem2.mat', 'r') as file:\n",
    "        data_sem2 = file[val_name][:]\n",
    "    data_sem2_copy = np.zeros((data_sem2.shape[0],time_window*fs_ratio,data_sem2.shape[2],data_sem2.shape[3]))\n",
    "    for i in range(18):\n",
    "        if liv_nonliv_raw_answer[i+18] == 0:\n",
    "            data_sem2_copy[i,:,:,:] = data_sem2[i,fs_ratio*q_offset[0]:fs_ratio*(q_offset[0]+time_window),:,:].copy()\n",
    "        if liv_nonliv_raw_answer[i+18] == 1:\n",
    "            data_sem2_copy[i,:,:,:] = data_sem2[i,fs_ratio*q_offset[1]:fs_ratio*(q_offset[1]+time_window),:,:].copy()\n",
    "    del data_sem2\n",
    "    with h5py.File(dirs+'sem5.mat', 'r') as file:\n",
    "        data_sem5 = file[val_name][:]\n",
    "    data_sem5_copy = np.zeros((data_sem5.shape[0],time_window*fs_ratio,data_sem5.shape[2],data_sem5.shape[3]))\n",
    "    for i in range(18):\n",
    "        if liv_nonliv_raw_answer[i+36] == 0:\n",
    "            data_sem5_copy[i,:,:,:] = data_sem5[i,fs_ratio*q_offset[0]:fs_ratio*(q_offset[0]+time_window),:,:].copy()\n",
    "        if liv_nonliv_raw_answer[i+36] == 1:\n",
    "            data_sem5_copy[i,:,:,:] = data_sem5[i,fs_ratio*q_offset[1]:fs_ratio*(q_offset[1]+time_window),:,:].copy()\n",
    "    del data_sem5\n",
    "    with h5py.File(dirs+'sem6.mat', 'r') as file:\n",
    "        data_sem6 = file[val_name][:]\n",
    "    data_sem6_copy = np.zeros((data_sem6.shape[0],time_window*fs_ratio,data_sem6.shape[2],data_sem6.shape[3]))\n",
    "    for i in range(18):\n",
    "        if liv_nonliv_raw_answer[i+54]== 0:\n",
    "            data_sem6_copy[i,:,:,:] = data_sem6[i,fs_ratio*q_offset[0]:fs_ratio*(q_offset[0]+time_window),:,:].copy()\n",
    "        if liv_nonliv_raw_answer[i+54] == 1:\n",
    "            data_sem6_copy[i,:,:,:] = data_sem6[i,fs_ratio*q_offset[1]:fs_ratio*(q_offset[1]+time_window),:,:].copy()\n",
    "    del data_sem6\n",
    "\n",
    "    liv_nonliv_raw_data = np.concatenate((data_sem1_copy, data_sem2_copy,data_sem5_copy,data_sem6_copy), axis=0)\n",
    "    print(\"liv_nonliv_load_complete. || shape : \",   liv_nonliv_raw_data.shape)\n",
    "\n",
    "    with h5py.File(dirs+'sem3.mat', 'r') as file:\n",
    "        data_sem3 = file[val_name][:]\n",
    "    data_sem3_copy = np.zeros((data_sem3.shape[0],time_window*fs_ratio,data_sem3.shape[2],data_sem3.shape[3]))\n",
    "    for i in range(18):\n",
    "        if body_nonbody_raw_answer[i] == 0:\n",
    "            data_sem3_copy[i,:,:,:] = data_sem3[i,fs_ratio*q_offset[2]:fs_ratio*(q_offset[2]+time_window),:,:].copy()\n",
    "        if body_nonbody_raw_answer[i] == 1:\n",
    "            data_sem3_copy[i,:,:,:] = data_sem3[i,fs_ratio*q_offset[3]:fs_ratio*(q_offset[3]+time_window),:,:].copy()\n",
    "    del data_sem3\n",
    "    with h5py.File(dirs+'sem4.mat', 'r') as file:\n",
    "        data_sem4 = file[val_name][:]\n",
    "    data_sem4_copy = np.zeros((data_sem4.shape[0],time_window*fs_ratio,data_sem4.shape[2],data_sem4.shape[3]))\n",
    "    for i in range(18):\n",
    "        if body_nonbody_raw_answer[i+18] == 0:\n",
    "            data_sem4_copy[i,:,:,:] = data_sem4[i,fs_ratio*q_offset[2]:fs_ratio*(q_offset[2]+time_window),:,:].copy()\n",
    "        if body_nonbody_raw_answer[i+18] == 1:\n",
    "            data_sem4_copy[i,:,:,:] = data_sem4[i,fs_ratio*q_offset[3]:fs_ratio*(q_offset[3]+time_window),:,:].copy()\n",
    "    del data_sem4\n",
    "    with h5py.File(dirs+'sem7.mat', 'r') as file:\n",
    "        data_sem7 = file[val_name][:]\n",
    "    data_sem7_copy = np.zeros((data_sem7.shape[0],time_window*fs_ratio,data_sem7.shape[2],data_sem7.shape[3]))\n",
    "    for i in range(18):\n",
    "        if body_nonbody_raw_answer[i+36] == 0:\n",
    "            data_sem7_copy[i,:,:,:] = data_sem7[i,fs_ratio*q_offset[2]:fs_ratio*(q_offset[2]+time_window),:,:].copy()\n",
    "        if body_nonbody_raw_answer[i+36] == 1:\n",
    "            data_sem7_copy[i,:,:,:] = data_sem7[i,fs_ratio*q_offset[3]:fs_ratio*(q_offset[3]+time_window),:,:].copy()\n",
    "    del data_sem7\n",
    "    with h5py.File(dirs+'sem8.mat', 'r') as file:\n",
    "        data_sem8 = file[val_name][:]\n",
    "    data_sem8_copy = np.zeros((data_sem8.shape[0],time_window*fs_ratio,data_sem8.shape[2],data_sem8.shape[3]))\n",
    "    for i in range(18):\n",
    "        if body_nonbody_raw_answer[i+54] == 0:\n",
    "            data_sem8_copy[i,:,:,:] = data_sem8[i,fs_ratio*q_offset[2]:fs_ratio*(q_offset[2]+time_window),:,:].copy()\n",
    "        if body_nonbody_raw_answer[i+54] == 1:\n",
    "            data_sem8_copy[i,:,:,:] = data_sem8[i,fs_ratio*q_offset[3]:fs_ratio*(q_offset[3]+time_window),:,:].copy()\n",
    "    del data_sem8\n",
    "\n",
    "    body_nonbody_raw_data = np.concatenate((data_sem3_copy, data_sem4_copy,data_sem7_copy,data_sem8_copy), axis=0)\n",
    "    print(\"body_nonbody_load_complete. || shape : \",   body_nonbody_raw_data.shape)\n",
    "    \n",
    "    return liv_nonliv_raw_data, body_nonbody_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87ffc8-2c70-4fb8-87a7-9d6e7566cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer giving file\n",
    "# liv category - 강아지 : puppy, 곰: bear, 사람 : haman,\n",
    "# nonliv category - 눈사람 : snowman, 책 : book, 모자 : hat\n",
    "# bodypart category - 코 : nose, 눈 : eye, 손 :hand\n",
    "# nonbodypart category - 꽃 : flower, 총 : gun, 컵: cup\n",
    "sem1_answers=['강아지','곰','눈사람','모자','강아지','책','곰','모자','사람','눈사람','곰','책','강아지','눈사람','사람','책','모자','사람']\n",
    "sem2_answers=['사람','곰','모자','강아지','곰','눈사람','사람','책','모자','강아지','눈사람','책','강아지','책','눈사람','사람','곰','모자']\n",
    "sem3_answers=['눈','손','꽃','컵','눈','총','손','컵','코','꽃','손','총','눈','꽃',\"코\",\"총\",\"컵\",\"코\"]\n",
    "sem4_answers=['코','손','컵','눈','손','꽃','코','총','컵','눈','꽃','총','눈','총',\"꽃\",'코','손','컵']\n",
    "sem5_answers=['눈사람','사람','강아지','모자','눈사람','곰','책','강아지','사람','모자','곰','책','곰','모자','사람','강아지','눈사람','책']\n",
    "sem6_answers=['책','강아지','사람','눈사람','모자','곰','강아지','곰','책','사람','곰','모자','책','눈사람','강아지','모자','사람','눈사람']\n",
    "sem7_answers=['꽃','코','눈','컵','꽃','손','총','눈','코','컵','손','총','손','컵','코','눈','꽃','총']\n",
    "sem8_answers=['총','눈','코','꽃','컵','손','눈','손','총','코','손','컵','총','꽃','눈','컵','코','꽃']\n",
    "\n",
    "\n",
    "\n",
    "#binary giver\n",
    "def liv_nonliv_answer_giver(answer_list):\n",
    "    empty_return=[]\n",
    "    for answer in answer_list:\n",
    "        if answer in ['사람','곰','강아지']:\n",
    "            empty_return.append(0)\n",
    "        else: #모자, 눈사람, 책\n",
    "            empty_return.append(1)\n",
    "    np_return = np.array(empty_return)\n",
    "\n",
    "    return np_return\n",
    "\n",
    "def body_nonbody_answer_giver(answer_list):\n",
    "    empty_return=[]\n",
    "    for answer in answer_list:\n",
    "        if answer in ['손','눈','코']:\n",
    "            empty_return.append(0)\n",
    "        else: #컵, 총, 꽃\n",
    "            empty_return.append(1)\n",
    "    np_return = np.array(empty_return)\n",
    "\n",
    "    return np_return\n",
    "    \n",
    "liv_nonliv_raw_answer = liv_nonliv_answer_giver(sem1_answers+sem2_answers+sem5_answers+sem6_answers)\n",
    "\n",
    "body_nonbody_raw_answer = body_nonbody_answer_giver(sem3_answers+sem4_answers+sem7_answers+sem8_answers)\n",
    "\n",
    "def l_each_answer_list_giver(answer_list):\n",
    "    anslist=[]\n",
    "    for ansnum in range(len(answer_list)):\n",
    "        if answer_list[ansnum] == \"사람\":\n",
    "            anslist.append(0)\n",
    "        if answer_list[ansnum] == \"곰\":\n",
    "            anslist.append(1)\n",
    "        if answer_list[ansnum] == \"강아지\":\n",
    "            anslist.append(2)\n",
    "        if answer_list[ansnum] == \"모자\":\n",
    "            anslist.append(3)\n",
    "        if answer_list[ansnum] == \"눈사람\":\n",
    "            anslist.append(4)\n",
    "        if answer_list[ansnum] == \"책\":\n",
    "            anslist.append(5)\n",
    "            \n",
    "    np_return = np.array(anslist)\n",
    "    \n",
    "    return np_return \n",
    "\n",
    "        \n",
    "def b_each_answer_list_giver(answer_list):\n",
    "    anslist=[]\n",
    "    for ansnum in range(len(answer_list)):\n",
    "        if answer_list[ansnum] == \"손\":\n",
    "            anslist.append(0)\n",
    "        if answer_list[ansnum] == \"눈\":\n",
    "            anslist.append(1)\n",
    "        if answer_list[ansnum] == \"코\":\n",
    "            anslist.append(2)\n",
    "        if answer_list[ansnum] == \"컵\":\n",
    "            anslist.append(3)\n",
    "        if answer_list[ansnum] == \"총\":\n",
    "            anslist.append(4)\n",
    "        if answer_list[ansnum] == \"꽃\":\n",
    "            anslist.append(5)\n",
    "    np_return = np.array(anslist)\n",
    "    \n",
    "    return np_return \n",
    "\n",
    "l_each_raw_answer = l_each_answer_list_giver(sem1_answers+sem2_answers+sem5_answers+sem6_answers)\n",
    "b_each_raw_answer = b_each_answer_list_giver(sem3_answers+sem4_answers+sem7_answers+sem8_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43e691-2e31-4a23-8bcf-322d8911ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_answer_not = np.concatenate((liv_nonliv_raw_answer,body_nonbody_raw_answer))\n",
    "total_answer_sess = np.concatenate((np.zeros(72), np.ones(72)))\n",
    "total_answer_4 = total_answer_not + total_answer_sess*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bc650-5547-4768-b36b-981a58b176b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_answer_4dim=np.zeros((144,4))\n",
    "for i in range(144):\n",
    "    total_answer_4dim[i,int(total_answer_4[i])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c5cb9-64ec-4d20-82e9-a82944b4cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_answer_4dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe52071-6b72-4bb7-a0c0-c902859864ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_answer = np.concatenate((np.array([total_answer_not, total_answer_sess]).T, total_answer_4dim),axis=1)\n",
    "print(total_answer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bb75b-c315-4a5d-9faa-5032c2bf40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x_data, y_data, split_ratio=0.2, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)  \n",
    "    \n",
    "    indices = np.arange(x_data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split_size = int(len(indices) * split_ratio)\n",
    "    test_indices = indices[:split_size]\n",
    "    train_indices = indices[split_size:]\n",
    "    \n",
    "    x_train = x_data[train_indices]\n",
    "    y_train = y_data[train_indices]\n",
    "    x_test = x_data[test_indices]\n",
    "    y_test = y_data[test_indices]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be271650-2e75-4369-a4b8-1f66126a35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_paired_data_plotter(data1, data2, data3, data4, rdata1, rdata2, rdata3, rdata4, iter_num, pat_name,freq_str,titlename,ch,goodpval_list, mode=\"r2\"):\n",
    "    plt.plot(np.mean(data1,axis=1), label='(liv+body) vs (nonliv+nonbody)', c='b')\n",
    "    plt.plot(np.mean(data2,axis=1), label ='(liv+nonliv) vs (body+nonbody) ', c='g')\n",
    "    plt.plot(np.mean(data3,axis=1), label ='2 x 2 ', c='r')\n",
    "    plt.plot(np.mean(data4,axis=1), label ='2 x 2, w.t. interaction', c='magenta')\n",
    "    plt.plot(np.mean(rdata1,axis=1),  c='b', linestyle = \"--\")\n",
    "    plt.plot(np.mean(rdata2,axis=1), c='g', linestyle = \"--\")\n",
    "    plt.plot(np.mean(rdata3,axis=1),  c='r', linestyle = \"--\")\n",
    "    plt.plot(np.mean(rdata4,axis=1),  c='magenta', linestyle = \"--\")\n",
    "    tick_positions = np.arange(0, 48)\n",
    "    freq_tick_value=[i*50 for i in range(1,49) ]\n",
    "    title = titlename +\" | \"+pat_name +' ch'+str(ch)+' '+ freq_str\n",
    "    plt.title(title ,fontsize=15)\n",
    "    plt.ylabel(mode,fontsize=14)\n",
    "    plt.xticks(ticks=tick_positions, labels=freq_tick_value,fontsize=13)\n",
    "    plt.xlabel('Time (ms); center of window',fontsize=14)\n",
    "    z_value = 1.96\n",
    "    sem_data_1 = np.std(data1, axis=1) / np.sqrt(iter_num)\n",
    "    sem_data_2 = np.std(data2, axis=1) / np.sqrt(iter_num)\n",
    "    sem_data_3 = np.std(data3, axis=1) / np.sqrt(iter_num)\n",
    "    sem_data_4 = np.std(data4, axis=1) / np.sqrt(iter_num)\n",
    "    sem_rdata_1 = np.std(rdata1, axis=1) / np.sqrt(iter_num)\n",
    "    sem_rdata_2 = np.std(rdata2, axis=1) / np.sqrt(iter_num)\n",
    "    sem_rdata_3 = np.std(rdata3, axis=1) / np.sqrt(iter_num)\n",
    "    sem_rdata_4 = np.std(rdata4, axis=1) / np.sqrt(iter_num)\n",
    "    \n",
    "    ci_lower_1 = np.mean(data1,axis=1) - z_value * sem_data_1\n",
    "    ci_upper_1 = np.mean(data1,axis=1) + z_value * sem_data_1\n",
    "    ci_lower_2 = np.mean(data2,axis=1) - z_value * sem_data_2\n",
    "    ci_upper_2 = np.mean(data2,axis=1) + z_value * sem_data_2\n",
    "    ci_lower_3 = np.mean(data3,axis=1) - z_value * sem_data_3\n",
    "    ci_upper_3 = np.mean(data3,axis=1) + z_value * sem_data_3\n",
    "    ci_lower_4 = np.mean(data4,axis=1) - z_value * sem_data_4\n",
    "    ci_upper_4 = np.mean(data4,axis=1) + z_value * sem_data_4\n",
    "\n",
    "    rci_lower_1 = np.mean(rdata1,axis=1) - z_value * sem_rdata_1\n",
    "    rci_upper_1 = np.mean(rdata1,axis=1) + z_value * sem_rdata_1\n",
    "    rci_lower_2 = np.mean(rdata2,axis=1) - z_value * sem_rdata_2\n",
    "    rci_upper_2 = np.mean(rdata2,axis=1) + z_value * sem_rdata_2\n",
    "    rci_lower_3 = np.mean(rdata3,axis=1) - z_value * sem_rdata_3\n",
    "    rci_upper_3 = np.mean(rdata3,axis=1) + z_value * sem_rdata_3\n",
    "    rci_lower_4 = np.mean(rdata4,axis=1) - z_value * sem_rdata_4\n",
    "    rci_upper_4 = np.mean(rdata4,axis=1) + z_value * sem_rdata_4\n",
    "    if mode == 'r':\n",
    "        max_point = max(np.max(np.array([np.mean(data1,axis=1),np.mean(data2,axis=1),np.mean(data3,axis=1),np.mean(data4,axis=1)]) ),0.6) + 0.1\n",
    "    else :\n",
    "        max_point = max(np.max(np.array([np.mean(data1,axis=1),np.mean(data2,axis=1),np.mean(data3,axis=1),np.mean(data4,axis=1)]) ),0.4) + 0.1\n",
    "    for t in range(11):\n",
    "        if goodpval_list[0][t] == 1:\n",
    "            plt.plot(t,  max_point - 0.04 , marker='*', color='b', markersize=3)\n",
    "        if goodpval_list[1][t] == 1:\n",
    "            plt.plot(t,  max_point - 0.03, marker='*', color='g', markersize=3 ) \n",
    "        if goodpval_list[2][t] == 1:\n",
    "            plt.plot(t, max_point - 0.02, marker='*', color='r', markersize=3 ) \n",
    "        if goodpval_list[3][t] == 1:\n",
    "            plt.plot(t, max_point - 0.01, marker='*', color='magenta', markersize=4) \n",
    "    plt.fill_between(range(len(data1)), ci_lower_1, ci_upper_1, color='b', alpha=0.2)\n",
    "    plt.fill_between(range(len(data2)), ci_lower_2, ci_upper_2, color='g', alpha=0.2)\n",
    "    plt.fill_between(range(len(data3)), ci_lower_3, ci_upper_3, color='r', alpha=0.2)\n",
    "    plt.fill_between(range(len(data4)), ci_lower_4, ci_upper_4, color='magenta', alpha=0.2)\n",
    "    plt.fill_between(range(len(rdata1)), rci_lower_1, rci_upper_1, color='b', alpha=0.1)\n",
    "    plt.fill_between(range(len(rdata2)), rci_lower_2, rci_upper_2, color='g', alpha=0.1)\n",
    "    plt.fill_between(range(len(rdata3)), rci_lower_3, rci_upper_3, color='r', alpha=0.1)\n",
    "    plt.fill_between(range(len(rdata4)), rci_lower_4, rci_upper_4, color='magenta', alpha=0.1)\n",
    "    if mode == 'r' :\n",
    "        plt.ylim(-0.05,max_point)\n",
    "    if mode == 'r2' :\n",
    "        plt.ylim(-0.1,max_point)\n",
    "    \n",
    "    if titlename ==\"R2 score (train)\":\n",
    "        savepath=\"encoding_graph_TR_A/\"+mode+\"_train_\"+pat_name +'_ch'+str(ch)+'_'+freq_str+'.png'\n",
    "    else:\n",
    "        savepath=\"encoding_graph_TR_/\"+mode+\"_\"+pat_name +'_ch'+str(ch)+'_'+freq_str+'.png'\n",
    "    plt.savefig(savepath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11651864-04b4-4195-99cf-2011f30593f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_t_value(data1, data2):\n",
    "    t_stat, p_value = ttest_ind(data1, data2, equal_var=False)  # 등분산성 가정 여부 설정\n",
    "    p_value_one_sided = p_value / 2 if t_stat > 0 else 1 - (p_value / 2)\n",
    "    return p_value_one_sided, t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206f1e3-1191-444c-ad4d-eb80c92687fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(a, b):\n",
    "    sorted_b = sorted(b, reverse=True)  # b를 복사하여 정렬\n",
    "    rank = 1\n",
    "    for num in sorted_b:\n",
    "        if a > num:\n",
    "            break\n",
    "        rank += 1\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f28138-77be-415e-9665-f790595e24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "from scipy.integrate import trapz\n",
    "def kde_fixed_h(samples, h_abs):\n",
    "    sigma = np.array(samples).std(ddof=1)\n",
    "    bw_factor = h_abs / sigma\n",
    "    return gaussian_kde(samples, bw_method=bw_factor)\n",
    "\n",
    "def get_KLdivergence(data1,ans1,data2,ans2,ans_num,itr_num):\n",
    "    ans1_list = [[],[],[],[]]\n",
    "    ans2_list = [[],[],[],[]]\n",
    "    for i in range(len(ans1)):\n",
    "        if ans1[i] == 0 :\n",
    "            ans1_list[0].append(i)\n",
    "        elif ans1[i] == 1 :\n",
    "            ans1_list[1].append(i)\n",
    "        elif ans1[i] == 2 :\n",
    "            ans1_list[2].append(i)\n",
    "        elif ans1[i] == 3 :\n",
    "            ans1_list[3].append(i)\n",
    "    for i in range(len(ans2)):\n",
    "        if ans2[i] == 0 :\n",
    "            ans2_list[0].append(i)\n",
    "        elif ans2[i] == 1:\n",
    "            ans2_list[1].append(i)\n",
    "        elif ans2[i] == 2:\n",
    "            ans2_list[2].append(i)\n",
    "        elif ans2[i] == 3:\n",
    "            ans2_list[3].append(i)\n",
    "    data1_list = [data1[ans1_list[k]] for k in range(ans_num)]\n",
    "    data2_list = [data2[ans2_list[k]] for k in range(ans_num)]\n",
    "    all_samples = np.concatenate([data1, data2])\n",
    "    h_global = 1.06 * all_samples.std(ddof=1) * len(all_samples) ** (-1/5)  # Silverman\n",
    "    \n",
    "    kde1_list = [kde_fixed_h(arr, h_global) for arr in data1_list]\n",
    "    kde2_list = [kde_fixed_h(arr, h_global) for arr in data2_list]\n",
    "\n",
    "    pi_t1   = np.array([len(ans1_list[i]) for i in range(ans_num)]) / np.sum(np.array([len(ans1_list[i]) for i in range(ans_num)]))\n",
    "    pi_t2   = np.array([len(ans2_list[i]) for i in range(ans_num)]) / np.sum(np.array([len(ans2_list[i]) for i in range(ans_num)]))\n",
    "\n",
    "    xmin_raw = np.min(all_samples)\n",
    "    xmax_raw = np.max(all_samples)\n",
    "    margin = 3 * h_global\n",
    "    xmin, xmax = xmin_raw - margin, xmax_raw + margin\n",
    "    grid = np.linspace(xmin, xmax, 500)\n",
    "    \n",
    "    eps = 1e-12                                # log(0) 방지용\n",
    "    kl_total = 0.0\n",
    "\n",
    "    for k in range(ans_num):\n",
    "      for k_ in range(ans_num):\n",
    "        kl_diff = 0 \n",
    "        if k_ == k:\n",
    "            p = kde1_list[k](grid) + eps             # f1ᵏ(x)\n",
    "            q = kde2_list[k_](grid) + eps             # f2ᵏ(x)\n",
    "            kl_k = trapz(p * np.log(p / q), grid)  # ∫ p log p/q dx\n",
    "            kl_same = pi_t1[k] * (np.log(pi_t1[k]/pi_t2[k_]) + kl_k)\n",
    "        else:\n",
    "            p = kde1_list[k](grid) + eps             # f1ᵏ(x)\n",
    "            q = kde2_list[k_](grid) + eps             # f2ᵏ(x)\n",
    "            kl_k = trapz(p * np.log(p / q), grid)  # ∫ p log p/q dx\n",
    "            kl_diff += pi_t1[k] * (np.log(pi_t1[k]/pi_t2[k_]) + kl_k)\n",
    "        kl_total += kl_same - kl_diff / (ans_num -1)\n",
    "    \n",
    "    #print(f\"delta KL = {kl_total:.6f}\")\n",
    "    randkl_list=[]\n",
    "    for rand_itr in range(itr_num):\n",
    "        ans2_shuffled = np.random.permutation(ans2)\n",
    "        #print(ans2_shuffled)\n",
    "        randans2_list = [[],[],[],[]]\n",
    "        for i in range(len(ans2_shuffled)):\n",
    "            if ans2_shuffled[i] == 0:\n",
    "                randans2_list[0].append(i)\n",
    "            if ans2_shuffled[i] == 1:\n",
    "                randans2_list[1].append(i)\n",
    "            if ans2_shuffled[i] == 2:\n",
    "                randans2_list[2].append(i)\n",
    "            if ans2_shuffled[i] == 3:\n",
    "                randans2_list[3].append(i)\n",
    "        #print(randans2_list)\n",
    "        randdata2_list = [data2[randans2_list[k]] for k in range(ans_num)]\n",
    "        randkde2_list = [kde_fixed_h(arr,h_global) for arr in randdata2_list]\n",
    "        eps = 1e-12                                # log(0) 방지용\n",
    "        kl_total_ = 0.0\n",
    "        for k in range(ans_num):\n",
    "          for k_ in range(ans_num):\n",
    "            kl_diff_ = 0.0 \n",
    "            if k == k_:\n",
    "                p = kde1_list[k](grid) + eps             # f1ᵏ(x)\n",
    "                q = randkde2_list[k_](grid) + eps             # f2ᵏ(x)\n",
    "                kl_k = trapz(p * np.log(p / q), grid)  # ∫ p log p/q dx\n",
    "                kl_same_= pi_t1[k] * (np.log(pi_t1[k]/pi_t2[k_]) + kl_k)\n",
    "            else:\n",
    "                p = kde1_list[k](grid) + eps             # f1ᵏ(x)\n",
    "                q = randkde2_list[k_](grid) + eps             # f2ᵏ(x)\n",
    "                kl_k = trapz(p * np.log(p / q), grid)  # ∫ p log p/q dx\n",
    "                kl_diff_ += pi_t1[k] * (np.log(pi_t1[k]/pi_t2[k_]) + kl_k)\n",
    "            kl_total_ += kl_same_ - kl_diff_ / (ans_num - 1)                       \n",
    "        randkl_list.append(kl_total_)\n",
    "    return kl_total, randkl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c09c6f-3ffc-4751-80bd-6d946b64ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../topomap/left_bestmodel_index.pickle', 'rb') as f:\n",
    "    left_bestmodel_index = pickle.load(f)\n",
    "\n",
    "with open('../topomap/right_bestmodel_index.pickle', 'rb') as f:\n",
    "    right_bestmodel_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33f6b4-7c80-47ef-ae09-b43a46d2a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_bestmodel_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85e30b-5d99-4159-8d2b-b90d091899f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fe29c-31f5-4fa9-b7c1-7e89766836a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import percentileofscore\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "total_subjects=['sub1','sub2','sub3','sub4','sub5','sub6','sub7','sub8','sub9','sub10','sub11','sub12','sub13','sub14','sub15']\n",
    "\n",
    "lefthemi_subjects = ['sub1','sub2','sub3','sub4','sub5','sub6','sub10','sub13','sub14','sub15']\n",
    "righthemi_subjects = ['sub3','sub7','sub8','sub9','sub11','sub12']\n",
    "lefthemi_subjects_startnum = [0,47,113,114,177,233,313,325,373,401]\n",
    "\n",
    "righthemi_subjects_startnum = [0,51,75,116,139,154]\n",
    "\n",
    "ver=11\n",
    "itr_num = 50000 \n",
    "np.random.seed(42)  # 고정된 랜덤 시드 설정\n",
    "random_seeds = np.random.randint(0, 50000, size=itr_num)\n",
    "old_threshold = 0.05 / 11 / 624 / 2\n",
    "new_threshold = 0.05 / 48 / 624 / 2\n",
    "# 첫 10개 확인\n",
    "tot_timecnt = 48\n",
    "freq_name = ['theta','alpha','beta','LG1','LG2','HG']\n",
    "for pat_name in total_subjects:\n",
    "    print(pat_name)\n",
    "            \n",
    "    r_old_path = \"save_dir/\"+pat_name+\"_corr_r.pkl\"\n",
    "    with open(r_old_path, 'rb') as f:\n",
    "        pat_r_data_old_raw = pickle.load(f)\n",
    "    pat_r_data_old = np.mean(pat_r_data_old_raw,axis=5)\n",
    "    print(pat_r_data_old.shape)\n",
    "    \n",
    "    r_old_path = \"save_dir/\"+pat_name+\"_r2score_test.pkl\"\n",
    "    with open(r_old_path, 'rb') as f:\n",
    "        pat_r2_data_old_raw = pickle.load(f)\n",
    "    pat_r2_data_old = np.mean(pat_r2_data_old_raw,axis=5)\n",
    "    print(pat_r_data_old.shape)    \n",
    "    \n",
    "    p_old_path = \"save_dir/\"+pat_name+\"_p_and_t.pkl\"\n",
    "    with open(p_old_path, 'rb') as f:\n",
    "        pat_p_data_old_raw = pickle.load(f)\n",
    "    pat_p_data_old = pat_p_data_old_raw[0][1,:,:,:,:]\n",
    "    print(pat_p_data_old.shape)\n",
    "\n",
    "    coef_old_path = \"save_dir/\"+pat_name+\"_r_coef.pkl\"\n",
    "    with open(coef_old_path, 'rb') as f:\n",
    "        pat_r_coef = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    coef_old_rand_path = \"save_dir/\"+pat_name+\"_r_random_coef.pkl\"\n",
    "    with open(coef_old_rand_path, 'rb') as f:\n",
    "        pat_r_random_coef = pickle.load(f)\n",
    "    pat_r_random_coef = pat_r_random_coef\n",
    "    \n",
    "    \n",
    "    bad_trial_for_chs = bad_trial_giver(pat_name,strip_grid_no_list)\n",
    "    error_trial = liv_nonliv_badnull_ans_dict[pat_name][0]+liv_nonliv_badnull_ans_dict[pat_name][1]+body_nonbody_badnull_ans_dict[pat_name][0]+body_nonbody_badnull_ans_dict[pat_name][1]\n",
    "    l_data_TR_A_raw, b_data_TR_A_raw = file_loader_and_npconcate_TFmean(pat_name,8000,10400,ver)\n",
    "    l_data_Q, b_data_Q = file_loader_and_npconcate_question_TFmean(pat_name,q_offsettiming,liv_nonliv_raw_answer,body_nonbody_raw_answer,ver)\n",
    "    \n",
    "    print(l_data_Q.shape)\n",
    "    l_data_Q_raw = l_data_Q[:,1100:,:,:]\n",
    "    b_data_Q_raw = b_data_Q[:,1100:,:,:]\n",
    "    l_data_TR_A = np.concatenate((l_data_Q_raw,l_data_TR_A_raw),axis=1)\n",
    "    b_data_TR_A = np.concatenate((b_data_Q_raw,b_data_TR_A_raw),axis=1)\n",
    "    raw_data_Q = np.concatenate((l_data_Q,b_data_Q))\n",
    "    good_ch = getPatgoodch(pat_name, strip_grid_no_list)\n",
    "    raw_data_TR_A = np.concatenate((l_data_TR_A, b_data_TR_A))\n",
    "    raw_data_TR_A = raw_data_TR_A[:,:,good_ch,:]\n",
    "    pat_random_kl = np.zeros((len(good_ch),itr_num))\n",
    "    pat_kl = np.zeros((len(good_ch),tot_timecnt))\n",
    "    type_index=[]\n",
    "\n",
    "    if pat_name != 'sub3':\n",
    "        if pat_name in lefthemi_subjects:\n",
    "            for ch in range(len(good_ch)):\n",
    "                sub_index = lefthemi_subjects.index(pat_name)\n",
    "                if ch + lefthemi_subjects_startnum[sub_index] in left_bestmodel_index[0]:\n",
    "                    type_index.append(0)\n",
    "                elif ch + lefthemi_subjects_startnum[sub_index] in left_bestmodel_index[1]:\n",
    "                    type_index.append(1)\n",
    "                elif ch + lefthemi_subjects_startnum[sub_index] in left_bestmodel_index[2]:\n",
    "                    type_index.append(2)\n",
    "                elif ch + lefthemi_subjects_startnum[sub_index] in left_bestmodel_index[3]:\n",
    "                    type_index.append(3)\n",
    "        if pat_name in righthemi_subjects:\n",
    "            for ch in range(len(good_ch)):\n",
    "                sub_index = righthemi_subjects.index(pat_name)\n",
    "                if ch + righthemi_subjects_startnum[sub_index] in right_bestmodel_index[0]:\n",
    "                    type_index.append(0)\n",
    "                elif ch + righthemi_subjects_startnum[sub_index] in right_bestmodel_index[1]:\n",
    "                    type_index.append(1)\n",
    "                elif ch + righthemi_subjects_startnum[sub_index] in right_bestmodel_index[2]:\n",
    "                    type_index.append(2)\n",
    "                elif ch + righthemi_subjects_startnum[sub_index] in right_bestmodel_index[3]:\n",
    "                    type_index.append(3)\n",
    "    elif pat_name == \"sub3\": #sub3 has electrodes both on right and left hemi\n",
    "        for ch in range(len(good_ch)):\n",
    "            if ch < 40:\n",
    "                sub_index = righthemi_subjects.index(pat_name)\n",
    "                if ch + righthemi_subjects_startnum[sub_index] in right_bestmodel_index[0]:\n",
    "                    type_index.append(0)\n",
    "                elif ch + righthemi_subjects_startnum[sub_index] in right_bestmodel_index[1]:\n",
    "                    type_index.append(1)\n",
    "                elif ch + righthemi_subjects_startnum[sub_index] in right_bestmodel_index[2]:\n",
    "                    type_index.append(2)\n",
    "                elif ch + righthemi_subjects_startnum[sub_index] in right_bestmodel_index[3]:\n",
    "                    type_index.append(3)\n",
    "            elif ch in [40,41]:\n",
    "                sub_index = lefthemi_subjects.index(pat_name)\n",
    "                if ch + lefthemi_subjects_startnum[sub_index] in left_bestmodel_index[0]:\n",
    "                    type_index.append(0)\n",
    "                elif ch + lefthemi_subjects_startnum[sub_index] in left_bestmodel_index[1]:\n",
    "                    type_index.append(1)\n",
    "                elif ch + lefthemi_subjects_startnum[sub_index] in left_bestmodel_index[2]:\n",
    "                    type_index.append(2)\n",
    "                elif ch + lefthemi_subjects_startnum[sub_index] in left_bestmodel_index[3]:\n",
    "                    type_index.append(3)\n",
    "            else:\n",
    "                sub_index = righthemi_subjects.index(pat_name)\n",
    "                if ch + righthemi_subjects_startnum[sub_index]+2 in right_bestmodel_index[0]:\n",
    "                    type_index.append(0)\n",
    "                elif ch + righthemi_subjects_startnum[sub_index]+2 in right_bestmodel_index[1]:\n",
    "                    type_index.append(1)\n",
    "                elif ch + righthemi_subjects_startnum[sub_index]+2 in right_bestmodel_index[2]:\n",
    "                    type_index.append(2)\n",
    "                elif ch + righthemi_subjects_startnum[sub_index]+2 in right_bestmodel_index[3]:\n",
    "                    type_index.append(3)\n",
    "                    \n",
    "    total_old_max_r_index = np.zeros((6,len(good_ch),4))\n",
    "    for freq in range(5,4,-1): #only for HG\n",
    "        print(\"================\",freq_name[freq],\"================\")\n",
    "        \n",
    "        for ch in range(len(good_ch)):\n",
    "            print(\"|\",end='')\n",
    "            bad_trial_total_time = [[] for i in range(tot_timecnt)]\n",
    "            for bins_ in range(-1,6):\n",
    "                if bins_ ==-1:\n",
    "                    t_start = 0\n",
    "                    t_end = 1\n",
    "                else:\n",
    "                    t_start = 8*bins_\n",
    "                    t_end = min(48,8*bins_+9)            \n",
    "                for t_cnt in range(t_start,t_end):\n",
    "                    bad_trial_total_time[t_cnt] += bad_trial_for_chs[bins_+1][ch]\n",
    "            \n",
    "            each_r_data_old = pat_r_data_old[0,:,freq,ch,:]\n",
    "            each_p_data_old = pat_p_data_old[:,freq,ch,:]\n",
    "            each_p_data_cutted_old = np.where(each_p_data_old<old_threshold,1,0)\n",
    "            old_max_r_index = []\n",
    "            for it in range(4):\n",
    "                if np.sum(each_p_data_cutted_old[it,:]) < 1 :\n",
    "                    old_max_r_index.append(-1)\n",
    "                else:\n",
    "                    p_filtered = each_r_data_old[it,:] * each_p_data_cutted_old[it,:]\n",
    "                    max_r_point_ = np.argmax(p_filtered)\n",
    "                    old_max_r_index.append(max_r_point_)\n",
    "            total_old_max_r_index[freq,ch,:] = np.array(old_max_r_index)\n",
    "            activity_total_list = []\n",
    "            for t in range(tot_timecnt):\n",
    "                raw_data_TR_t = raw_data_TR_A[:, 100*t : 100*t +200 , : , :]\n",
    "                data_TR_t = np.mean(raw_data_TR_t,axis=1)\n",
    "                bad_trial_TR = bad_trial_total_time[t]\n",
    "                good_trial_TR = [i for i in range(144) if i not in bad_trial_TR+error_trial]\n",
    "                bad_trial_Q = bad_trial_for_chs[0][ch]\n",
    "                good_trial_Q = [i for i in range(144) if i not in bad_trial_Q+error_trial]\n",
    "                data_good_TR_t = data_TR_t[:,ch,freq][good_trial_Q].tolist()\n",
    "                activity_total_list += data_good_TR_t\n",
    "\n",
    "                for types in range(4):\n",
    "                  if types == type_index[ch]:\n",
    "                    if old_max_r_index[types] != -1:\n",
    "                        data1_ = raw_data_Q[:,old_max_r_index[types]*100:old_max_r_index[types]*100+200,:,:]\n",
    "                        data1_ = np.mean(data1_,axis=1)\n",
    "                        if types == 0 :\n",
    "                            tot_answer = total_answer_not\n",
    "                            ans_num = 2\n",
    "                        if types == 1 :\n",
    "                            tot_answer = total_answer_sess\n",
    "                            ans_num = 2\n",
    "                        if types in [2,3]:\n",
    "                            tot_answer = total_answer_4\n",
    "                            ans_num = 4\n",
    "                        \n",
    "                        data1 = data1_[:,ch,freq][good_trial_Q]\n",
    "                        ans1 = tot_answer[good_trial_Q]\n",
    "                        data2 = data_TR_t[:,ch,freq][good_trial_TR]\n",
    "                        ans2 = tot_answer[good_trial_TR]\n",
    "                        kl, randkl_list = get_KLdivergence(data1,ans1,data2,ans2,ans_num,itr_num)\n",
    "                        pat_kl[ch, t] = kl\n",
    "                        pat_random_kl[ch,t,:] = np.array(randkl_list)\n",
    "                        print(percentileofscore(pat_random_kl[ch,t,:], pat_kl[ch, t], kind=\"weak\"), end=' ')\n",
    "        save_file_name = pat_name + \"_kldevergence_diff.pkl\"\n",
    "        with open(save_file_name, 'wb') as f:\n",
    "            pickle.dump([pat_kl,pat_random_kl], f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
