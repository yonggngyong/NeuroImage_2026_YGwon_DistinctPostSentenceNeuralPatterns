{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92357824-74b3-4bba-9ad4-6a93954ba2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8ec81-bd4f-4c5e-b176-dbadeefdfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_subjects=['sub1','sub2','sub3','sub4','sub5','sub6','sub7','sub8','sub9','sub10','sub11','sub12','sub13','sub14','sub15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7aa2d-6c2b-4d37-8973-0e8fcc10ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_grid_no_list={'sub1':[0,1,2,3],\n",
    "                    'sub2':[0,1,2,3,4,5],\n",
    "                    'sub3':[0,1,2,3,4],\n",
    "                    'sub4':[0,1,2,3,4],\n",
    "                    'sub5':[0,1,2,3],\n",
    "                    'sub6':[0,1,2,3,4],\n",
    "                    'sub7':[0,1,2],\n",
    "                    'sub8':[0,2,3],\n",
    "                    'sub9':[0,1,2,3],\n",
    "                    'sub10':[0,1],\n",
    "                    'sub11':[0,1,2],\n",
    "                    'sub12':[0,1,2],\n",
    "                    'sub13':[0,1,2,3],\n",
    "                    'sub14':[0,1,2,3],\n",
    "                    'sub15':[0,1,2,3,4]\n",
    "}\n",
    "                   \n",
    "\n",
    "# Among the two lists in badnull_ans_dict, the first contains cases where the answer was stated incorrectly (i.e., the opposite of the correct answer), and the second contains experimental errors—such as failing to give an answer, speaking during the baseline period, or similar issues.\n",
    "liv_nonliv_badnull_ans_dict = {'sub1': [[], []],\n",
    "                               'sub2': [[], []],\n",
    "                               'sub3': [[49], []],\n",
    "                               'sub4': [[], []],\n",
    "                               'sub5': [[5, 7], []],\n",
    "                               'sub6': [[41], [17]],\n",
    "                               'sub7': [[], []],\n",
    "                               'sub8': [[], [33]],\n",
    "                               'sub9' : [[8],[13]],\n",
    "                               'sub10' : [[],[36]],\n",
    "                               'sub11' : [[],[]],\n",
    "                               'sub12' : [[],[]],\n",
    "                               'sub13' : [[16],[]],\n",
    "                               'sub14': [[10], [23]],\n",
    "                               'sub15': [[], []],\n",
    "                               }\n",
    "\n",
    "\n",
    "body_nonbody_badnull_ans_dict = {'sub1': [[], []],\n",
    "                               'sub2': [[], []],\n",
    "                               'sub3': [[], []],\n",
    "                               'sub4': [[], [3,7]],\n",
    "                               'sub5': [[], []],\n",
    "                               'sub6': [[], []],\n",
    "                               'sub7': [[], []],\n",
    "                               'sub8': [[], []],\n",
    "                               'sub9' : [[],[]],\n",
    "                               'sub10' : [[],[4]],\n",
    "                               'sub11' : [[],[]],\n",
    "                               'sub12' : [[],[]],\n",
    "                               'sub13' : [[],[]],\n",
    "                               'sub14': [[], [0]],\n",
    "                               'sub15': [[], []],\n",
    "                               }\n",
    "\n",
    "\n",
    "body_nonbody_badnull_ans_dict = {'sub1': [[], []],\n",
    "                                 'sub2': [[], []],\n",
    "                                 'sub3': [[], []],\n",
    "                                 'sub4': [[], [3+72, 7+72]],\n",
    "                                 'sub5': [[], []],\n",
    "                                 'sub6': [[], []],\n",
    "                                 'sub7': [[], []],\n",
    "                                 'sub8': [[], []],\n",
    "                                 'sub9' : [[],[]],\n",
    "                                 'sub10' : [[i for i in range(54,72)],[4+72]+[i for i in range(126,144)]],\n",
    "                                 'sub11' : [[],[]],\n",
    "                                 'sub12' : [[],[]],\n",
    "                                 'sub13' : [[],[]],\n",
    "                                 'sub14': [[], [0+72]],\n",
    "                                 'sub15': [[], []],\n",
    "                                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac34bf0-9bcb-46fe-8513-ed5cb12d3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 72 in bodypart\n",
    "def bad_trial_giver(pat_name, strip_grid_no_list):\n",
    "    total_badtrial=[[],[],[],[],[],[],[]]\n",
    "    ch_info_dirs = 'data_path/'+pat_name+'/'+pat_name+'good_ch.mat'\n",
    "    with h5py.File(ch_info_dirs, 'r') as file:\n",
    "        ch_info_raw = file['good_ch_index'][:]\n",
    "    max_grid_group = int(ch_info_raw[-1]//1000)\n",
    "    ch_info = [[] for i in range(max_grid_group)]\n",
    "    for each_ch_info in ch_info_raw:\n",
    "        grid_group = int(each_ch_info//1000)-1\n",
    "        grid_num = int(each_ch_info%1000)\n",
    "        ch_info[grid_group].append(grid_num)\n",
    "    ch_indexing=[0]\n",
    "    cnter_ch=0\n",
    "    strip_grid_no = str(strip_grid_no_list[pat_name])\n",
    "    file_path = \"l_\"+pat_name+'_'+strip_grid_no+\"_index.pkl\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        checkbox_indexs = pickle.load(f)\n",
    "    print(checkbox_indexs)\n",
    "    file_path = \"badtrials/l_\"+pat_name+'_'+strip_grid_no+\".pkl\"\n",
    "    with open(file_path, 'rb') as f: #6\n",
    "        l_badtrial_raw = np.array(pickle.load(f))\n",
    "    \n",
    "    file_path = \"badtrials/b_\"+pat_name+'_'+strip_grid_no+\".pkl\"\n",
    "    with open(file_path, 'rb') as f: #7\n",
    "        b_badtrial_raw = np.array(pickle.load(f))    \n",
    "    \n",
    "    for i in ch_info:\n",
    "        ch_indexing.append(ch_indexing[cnter_ch]+len(i))\n",
    "        cnter_ch +=1\n",
    "    \n",
    "    for idx in range(len(strip_grid_no_list[pat_name])):\n",
    "        checkbox_index = checkbox_indexs[idx]\n",
    "        ch_cnt = len(ch_info[strip_grid_no_list[pat_name][idx]])\n",
    "        for point in range(7):\n",
    "            check_bullean_l = np.where(np.sum(l_badtrial_raw[checkbox_index,:,:][:,[0,point+1]],axis=1)>0,1,0)\n",
    "            check_bullean_b = np.where(np.sum(b_badtrial_raw[checkbox_index,:,:][:,[0,point+1]],axis=1)>0,1,0)\n",
    "            bad_trial_list = []\n",
    "            for i in range(72):\n",
    "                if check_bullean_l[i] == 1:\n",
    "                    bad_trial_list.append(i)\n",
    "            for i in range(72):\n",
    "                if check_bullean_b[i] == 1:\n",
    "                    bad_trial_list.append(i+72)\n",
    "            for i in range(ch_cnt):\n",
    "                total_badtrial[point].append(bad_trial_list)\n",
    "    return total_badtrial\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a50e6c-801c-4ceb-91ce-79bbb1d7563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatgoodch(pat_name,strip_grid_no_list):\n",
    "    strip_grid_no = strip_grid_no_list[pat_name]\n",
    "    return_list=[]\n",
    "    ch_info_dirs = 'data_path/'+pat_name+'/'+pat_name+'good_ch.mat'\n",
    "    with h5py.File(ch_info_dirs, 'r') as file:\n",
    "        ch_info_raw = file['good_ch_index'][:]\n",
    "    max_grid_group = int(ch_info_raw[-1]//1000)\n",
    "    ch_info = [[] for i in range(max_grid_group)]\n",
    "    for each_ch_info in ch_info_raw:\n",
    "        grid_group = int(each_ch_info//1000)-1\n",
    "        grid_num = int(each_ch_info%1000)\n",
    "        ch_info[grid_group].append(grid_num)\n",
    "    ch_indexing=[0]\n",
    "    cnter_ch=0\n",
    "    for i in ch_info:\n",
    "        #print(len(i))\n",
    "        ch_indexing.append(ch_indexing[cnter_ch]+len(i))\n",
    "        cnter_ch +=1\n",
    "    for strip_grid_idx in strip_grid_no:\n",
    "        for i in range(ch_indexing[strip_grid_idx],ch_indexing[strip_grid_idx+1]):\n",
    "            return_list.append(i)\n",
    "    print(ch_indexing)\n",
    "    print(len(ch_info_raw))\n",
    "    print(len(return_list))\n",
    "    return return_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8356b7-713f-4dbd-a6a8-5cbfa69c300d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FILE LOAD\n",
    "\n",
    "q_offsettiming=[3407,4070,3823,4302] #offset time for each sentence\n",
    "\n",
    "def file_loader_and_npconcate_TFmean(pat_name,cutstart,cutend,ver):\n",
    "    dirs = 'datapath/preprocessed/'+pat_name+'/'+pat_name+'TF_freqenv_ver'+str(ver)\n",
    "    val_name = \"TF_mean_epoch\"\n",
    "    fs_ratio = 2\n",
    "    with h5py.File(dirs+'sem1.mat', 'r') as file:\n",
    "        data_sem1 = file[val_name][:]\n",
    "    data_sem1_copy = data_sem1[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem1\n",
    "    with h5py.File(dirs+'sem2.mat', 'r') as file:\n",
    "        data_sem2 = file[val_name][:]\n",
    "    data_sem2_copy = data_sem2[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem2\n",
    "    with h5py.File(dirs+'sem5.mat', 'r') as file:\n",
    "        data_sem5 = file[val_name][:]\n",
    "    data_sem5_copy = data_sem5[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem5\n",
    "    with h5py.File(dirs+'sem6.mat', 'r') as file:\n",
    "        data_sem6 = file[val_name][:]\n",
    "    data_sem6_copy = data_sem6[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem6\n",
    "\n",
    "    liv_nonliv_raw_data = np.concatenate((data_sem1_copy, data_sem2_copy,data_sem5_copy,data_sem6_copy), axis=0)\n",
    "    print(\"liv_nonliv_load_complete. || shape : \",   liv_nonliv_raw_data.shape)\n",
    "\n",
    "    with h5py.File(dirs+'sem3.mat', 'r') as file:\n",
    "        data_sem3 = file[val_name][:]\n",
    "    data_sem3_copy = data_sem3[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem3\n",
    "    with h5py.File(dirs+'sem4.mat', 'r') as file:\n",
    "        data_sem4 = file[val_name][:]\n",
    "    data_sem4_copy = data_sem4[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem4\n",
    "    with h5py.File(dirs+'sem7.mat', 'r') as file:\n",
    "        data_sem7 = file[val_name][:]\n",
    "    data_sem7_copy = data_sem7[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem7\n",
    "    with h5py.File(dirs+'sem8.mat', 'r') as file:\n",
    "        data_sem8 = file[val_name][:]\n",
    "    data_sem8_copy = data_sem8[:, fs_ratio*cutstart:fs_ratio*cutend, :, :].copy()\n",
    "    del data_sem8\n",
    "\n",
    "    body_nonbody_raw_data = np.concatenate((data_sem3_copy, data_sem4_copy,data_sem7_copy,data_sem8_copy), axis=0)\n",
    "    print(\"body_nonbody_load_complete. || shape : \",   body_nonbody_raw_data.shape)\n",
    "    \n",
    "    return liv_nonliv_raw_data, body_nonbody_raw_data\n",
    "\n",
    "def file_loader_and_npconcate_question_TFmean(pat_name,q_offset,liv_nonliv_raw_answer,body_nonbody_raw_answer,ver):\n",
    "    time_window = 600\n",
    "    dirs = 'datapath/preprocessed/'+pat_name+'/'+pat_name+'TF_freqenv_ver'+str(ver)\n",
    "    val_name =\"TF_mean_epoch\"\n",
    "    fs_ratio = 2 \n",
    "    with h5py.File(dirs+'sem1.mat', 'r') as file:\n",
    "        data_sem1 = file[val_name][:]\n",
    "    data_sem1_copy = np.zeros((data_sem1.shape[0],time_window*fs_ratio,data_sem1.shape[2],data_sem1.shape[3]))\n",
    "    for i in range(18):\n",
    "        if liv_nonliv_raw_answer[i] == 0:\n",
    "            data_sem1_copy[i,:,:,:] = data_sem1[i,fs_ratio*q_offset[0]:fs_ratio*(q_offset[0]+time_window),:,:].copy()\n",
    "        if liv_nonliv_raw_answer[i] == 1:\n",
    "            data_sem1_copy[i,:,:,:] = data_sem1[i,fs_ratio*q_offset[1]:fs_ratio*(q_offset[1]+time_window),:,:].copy()\n",
    "    del data_sem1\n",
    "    with h5py.File(dirs+'sem2.mat', 'r') as file:\n",
    "        data_sem2 = file[val_name][:]\n",
    "    data_sem2_copy = np.zeros((data_sem2.shape[0],time_window*fs_ratio,data_sem2.shape[2],data_sem2.shape[3]))\n",
    "    for i in range(18):\n",
    "        if liv_nonliv_raw_answer[i+18] == 0:\n",
    "            data_sem2_copy[i,:,:,:] = data_sem2[i,fs_ratio*q_offset[0]:fs_ratio*(q_offset[0]+time_window),:,:].copy()\n",
    "        if liv_nonliv_raw_answer[i+18] == 1:\n",
    "            data_sem2_copy[i,:,:,:] = data_sem2[i,fs_ratio*q_offset[1]:fs_ratio*(q_offset[1]+time_window),:,:].copy()\n",
    "    del data_sem2\n",
    "    with h5py.File(dirs+'sem5.mat', 'r') as file:\n",
    "        data_sem5 = file[val_name][:]\n",
    "    data_sem5_copy = np.zeros((data_sem5.shape[0],time_window*fs_ratio,data_sem5.shape[2],data_sem5.shape[3]))\n",
    "    for i in range(18):\n",
    "        if liv_nonliv_raw_answer[i+36] == 0:\n",
    "            data_sem5_copy[i,:,:,:] = data_sem5[i,fs_ratio*q_offset[0]:fs_ratio*(q_offset[0]+time_window),:,:].copy()\n",
    "        if liv_nonliv_raw_answer[i+36] == 1:\n",
    "            data_sem5_copy[i,:,:,:] = data_sem5[i,fs_ratio*q_offset[1]:fs_ratio*(q_offset[1]+time_window),:,:].copy()\n",
    "    del data_sem5\n",
    "    with h5py.File(dirs+'sem6.mat', 'r') as file:\n",
    "        data_sem6 = file[val_name][:]\n",
    "    data_sem6_copy = np.zeros((data_sem6.shape[0],time_window*fs_ratio,data_sem6.shape[2],data_sem6.shape[3]))\n",
    "    for i in range(18):\n",
    "        if liv_nonliv_raw_answer[i+54]== 0:\n",
    "            data_sem6_copy[i,:,:,:] = data_sem6[i,fs_ratio*q_offset[0]:fs_ratio*(q_offset[0]+time_window),:,:].copy()\n",
    "        if liv_nonliv_raw_answer[i+54] == 1:\n",
    "            data_sem6_copy[i,:,:,:] = data_sem6[i,fs_ratio*q_offset[1]:fs_ratio*(q_offset[1]+time_window),:,:].copy()\n",
    "    del data_sem6\n",
    "\n",
    "    liv_nonliv_raw_data = np.concatenate((data_sem1_copy, data_sem2_copy,data_sem5_copy,data_sem6_copy), axis=0)\n",
    "    print(\"liv_nonliv_load_complete. || shape : \",   liv_nonliv_raw_data.shape)\n",
    "\n",
    "    with h5py.File(dirs+'sem3.mat', 'r') as file:\n",
    "        data_sem3 = file[val_name][:]\n",
    "    data_sem3_copy = np.zeros((data_sem3.shape[0],time_window*fs_ratio,data_sem3.shape[2],data_sem3.shape[3]))\n",
    "    for i in range(18):\n",
    "        if body_nonbody_raw_answer[i] == 0:\n",
    "            data_sem3_copy[i,:,:,:] = data_sem3[i,fs_ratio*q_offset[2]:fs_ratio*(q_offset[2]+time_window),:,:].copy()\n",
    "        if body_nonbody_raw_answer[i] == 1:\n",
    "            data_sem3_copy[i,:,:,:] = data_sem3[i,fs_ratio*q_offset[3]:fs_ratio*(q_offset[3]+time_window),:,:].copy()\n",
    "    del data_sem3\n",
    "    with h5py.File(dirs+'sem4.mat', 'r') as file:\n",
    "        data_sem4 = file[val_name][:]\n",
    "    data_sem4_copy = np.zeros((data_sem4.shape[0],time_window*fs_ratio,data_sem4.shape[2],data_sem4.shape[3]))\n",
    "    for i in range(18):\n",
    "        if body_nonbody_raw_answer[i+18] == 0:\n",
    "            data_sem4_copy[i,:,:,:] = data_sem4[i,fs_ratio*q_offset[2]:fs_ratio*(q_offset[2]+time_window),:,:].copy()\n",
    "        if body_nonbody_raw_answer[i+18] == 1:\n",
    "            data_sem4_copy[i,:,:,:] = data_sem4[i,fs_ratio*q_offset[3]:fs_ratio*(q_offset[3]+time_window),:,:].copy()\n",
    "    del data_sem4\n",
    "    with h5py.File(dirs+'sem7.mat', 'r') as file:\n",
    "        data_sem7 = file[val_name][:]\n",
    "    data_sem7_copy = np.zeros((data_sem7.shape[0],time_window*fs_ratio,data_sem7.shape[2],data_sem7.shape[3]))\n",
    "    for i in range(18):\n",
    "        if body_nonbody_raw_answer[i+36] == 0:\n",
    "            data_sem7_copy[i,:,:,:] = data_sem7[i,fs_ratio*q_offset[2]:fs_ratio*(q_offset[2]+time_window),:,:].copy()\n",
    "        if body_nonbody_raw_answer[i+36] == 1:\n",
    "            data_sem7_copy[i,:,:,:] = data_sem7[i,fs_ratio*q_offset[3]:fs_ratio*(q_offset[3]+time_window),:,:].copy()\n",
    "    del data_sem7\n",
    "    with h5py.File(dirs+'sem8.mat', 'r') as file:\n",
    "        data_sem8 = file[val_name][:]\n",
    "    data_sem8_copy = np.zeros((data_sem8.shape[0],time_window*fs_ratio,data_sem8.shape[2],data_sem8.shape[3]))\n",
    "    for i in range(18):\n",
    "        if body_nonbody_raw_answer[i+54] == 0:\n",
    "            data_sem8_copy[i,:,:,:] = data_sem8[i,fs_ratio*q_offset[2]:fs_ratio*(q_offset[2]+time_window),:,:].copy()\n",
    "        if body_nonbody_raw_answer[i+54] == 1:\n",
    "            data_sem8_copy[i,:,:,:] = data_sem8[i,fs_ratio*q_offset[3]:fs_ratio*(q_offset[3]+time_window),:,:].copy()\n",
    "    del data_sem8\n",
    "\n",
    "    body_nonbody_raw_data = np.concatenate((data_sem3_copy, data_sem4_copy,data_sem7_copy,data_sem8_copy), axis=0)\n",
    "    print(\"body_nonbody_load_complete. || shape : \",   body_nonbody_raw_data.shape)\n",
    "    \n",
    "    return liv_nonliv_raw_data, body_nonbody_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87ffc8-2c70-4fb8-87a7-9d6e7566cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer giving file\n",
    "# liv category - 강아지 : puppy, 곰: bear, 사람 : haman,\n",
    "# nonliv category - 눈사람 : snowman, 책 : book, 모자 : hat\n",
    "# bodypart category - 코 : nose, 눈 : eye, 손 :hand\n",
    "# nonbodypart category - 꽃 : flower, 총 : gun, 컵: cup\n",
    "sem1_answers=['강아지','곰','눈사람','모자','강아지','책','곰','모자','사람','눈사람','곰','책','강아지','눈사람','사람','책','모자','사람']\n",
    "sem2_answers=['사람','곰','모자','강아지','곰','눈사람','사람','책','모자','강아지','눈사람','책','강아지','책','눈사람','사람','곰','모자']\n",
    "sem3_answers=['눈','손','꽃','컵','눈','총','손','컵','코','꽃','손','총','눈','꽃',\"코\",\"총\",\"컵\",\"코\"]\n",
    "sem4_answers=['코','손','컵','눈','손','꽃','코','총','컵','눈','꽃','총','눈','총',\"꽃\",'코','손','컵']\n",
    "sem5_answers=['눈사람','사람','강아지','모자','눈사람','곰','책','강아지','사람','모자','곰','책','곰','모자','사람','강아지','눈사람','책']\n",
    "sem6_answers=['책','강아지','사람','눈사람','모자','곰','강아지','곰','책','사람','곰','모자','책','눈사람','강아지','모자','사람','눈사람']\n",
    "sem7_answers=['꽃','코','눈','컵','꽃','손','총','눈','코','컵','손','총','손','컵','코','눈','꽃','총']\n",
    "sem8_answers=['총','눈','코','꽃','컵','손','눈','손','총','코','손','컵','총','꽃','눈','컵','코','꽃']\n",
    "\n",
    "\n",
    "\n",
    "#binary giver\n",
    "def liv_nonliv_answer_giver(answer_list):\n",
    "    empty_return=[]\n",
    "    for answer in answer_list:\n",
    "        if answer in ['사람','곰','강아지']:\n",
    "            empty_return.append(0)\n",
    "        else: #모자, 눈사람, 책\n",
    "            empty_return.append(1)\n",
    "    np_return = np.array(empty_return)\n",
    "\n",
    "    return np_return\n",
    "\n",
    "def body_nonbody_answer_giver(answer_list):\n",
    "    empty_return=[]\n",
    "    for answer in answer_list:\n",
    "        if answer in ['손','눈','코']:\n",
    "            empty_return.append(0)\n",
    "        else: #컵, 총, 꽃\n",
    "            empty_return.append(1)\n",
    "    np_return = np.array(empty_return)\n",
    "\n",
    "    return np_return\n",
    "    \n",
    "liv_nonliv_raw_answer = liv_nonliv_answer_giver(sem1_answers+sem2_answers+sem5_answers+sem6_answers)\n",
    "\n",
    "body_nonbody_raw_answer = body_nonbody_answer_giver(sem3_answers+sem4_answers+sem7_answers+sem8_answers)\n",
    "\n",
    "def l_each_answer_list_giver(answer_list):\n",
    "    anslist=[]\n",
    "    for ansnum in range(len(answer_list)):\n",
    "        if answer_list[ansnum] == \"사람\":\n",
    "            anslist.append(0)\n",
    "        if answer_list[ansnum] == \"곰\":\n",
    "            anslist.append(1)\n",
    "        if answer_list[ansnum] == \"강아지\":\n",
    "            anslist.append(2)\n",
    "        if answer_list[ansnum] == \"모자\":\n",
    "            anslist.append(3)\n",
    "        if answer_list[ansnum] == \"눈사람\":\n",
    "            anslist.append(4)\n",
    "        if answer_list[ansnum] == \"책\":\n",
    "            anslist.append(5)\n",
    "            \n",
    "    np_return = np.array(anslist)\n",
    "    \n",
    "    return np_return \n",
    "\n",
    "        \n",
    "def b_each_answer_list_giver(answer_list):\n",
    "    anslist=[]\n",
    "    for ansnum in range(len(answer_list)):\n",
    "        if answer_list[ansnum] == \"손\":\n",
    "            anslist.append(0)\n",
    "        if answer_list[ansnum] == \"눈\":\n",
    "            anslist.append(1)\n",
    "        if answer_list[ansnum] == \"코\":\n",
    "            anslist.append(2)\n",
    "        if answer_list[ansnum] == \"컵\":\n",
    "            anslist.append(3)\n",
    "        if answer_list[ansnum] == \"총\":\n",
    "            anslist.append(4)\n",
    "        if answer_list[ansnum] == \"꽃\":\n",
    "            anslist.append(5)\n",
    "    np_return = np.array(anslist)\n",
    "    \n",
    "    return np_return \n",
    "\n",
    "l_each_raw_answer = l_each_answer_list_giver(sem1_answers+sem2_answers+sem5_answers+sem6_answers)\n",
    "b_each_raw_answer = b_each_answer_list_giver(sem3_answers+sem4_answers+sem7_answers+sem8_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43e691-2e31-4a23-8bcf-322d8911ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_answer_not = np.concatenate((liv_nonliv_raw_answer,body_nonbody_raw_answer))\n",
    "total_answer_sess = np.concatenate((np.zeros(72), np.ones(72)))\n",
    "total_answer_4 = total_answer_not + total_answer_sess*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bc650-5547-4768-b36b-981a58b176b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_answer_4dim=np.zeros((144,4))\n",
    "for i in range(144):\n",
    "    total_answer_4dim[i,int(total_answer_4[i])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c5cb9-64ec-4d20-82e9-a82944b4cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_answer_4dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe52071-6b72-4bb7-a0c0-c902859864ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_answer = np.concatenate((np.array([total_answer_not, total_answer_sess]).T, total_answer_4dim),axis=1)\n",
    "print(total_answer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bb75b-c315-4a5d-9faa-5032c2bf40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x_data, y_data, split_ratio=0.2, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)  \n",
    "    \n",
    "    indices = np.arange(x_data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split_size = int(len(indices) * split_ratio)\n",
    "    test_indices = indices[:split_size]\n",
    "    train_indices = indices[split_size:]\n",
    "    \n",
    "    x_train = x_data[train_indices]\n",
    "    y_train = y_data[train_indices]\n",
    "    x_test = x_data[test_indices]\n",
    "    y_test = y_data[test_indices]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02938d6b-a392-404c-90fb-fae7e443fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.zeros(20)\n",
    "plt.plot(example, c=(0.1, 0.7, 0.8))\n",
    "plt.plot(example+1, c = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be271650-2e75-4369-a4b8-1f66126a35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def four_paired_data_plotter(data1, data2, data3, data4, rdata1, rdata2, rdata3, rdata4, iter_num, pat_name,freq_str,titlename,ch,goodpval_list, mode=\"r2\"):\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.plot(np.mean(data1,axis=1), label='(liv+body) vs (nonliv+nonbody)', c='r')\n",
    "    plt.plot(np.mean(data2,axis=1), label ='(liv+nonliv) vs (body+nonbody) ', c='g')\n",
    "    cyan_c =(0.1, 0.7, 0.8)\n",
    "    plt.plot(np.mean(data3,axis=1), label ='2 x 2 ', c=cyan_c)\n",
    "    plt.plot(np.mean(data4,axis=1), label ='2 x 2, w.t. interaction', c='magenta')\n",
    "    plt.plot(np.mean(rdata1,axis=1),  c='r', linestyle = \"--\")\n",
    "    plt.plot(np.mean(rdata2,axis=1), c='g', linestyle = \"--\")\n",
    "    plt.plot(np.mean(rdata3,axis=1),  c=cyan_c, linestyle = \"--\")\n",
    "    plt.plot(np.mean(rdata4,axis=1),  c='magenta', linestyle = \"--\")\n",
    "    tick_positions = np.arange(0, 12)\n",
    "    freq_tick_value=[i*50 for i in range(1,13) ]\n",
    "    tick_positions = np.arange(0, 12, 2)\n",
    "    freq_tick_value=[i*50 for i in tick_positions+1 ]\n",
    "    title = titlename +pat_name +', ch'+str(ch)\n",
    "    plt.title(title ,fontsize=15)\n",
    "    plt.ylabel(mode,fontsize=14)\n",
    "    plt.xticks(ticks=tick_positions, labels=freq_tick_value,fontsize=13)\n",
    "    plt.xlabel('Time (ms); center of window',fontsize=14)\n",
    "    z_value = 1.96\n",
    "    sem_data_1 = np.std(data1, axis=1) / np.sqrt(iter_num)\n",
    "    sem_data_2 = np.std(data2, axis=1) / np.sqrt(iter_num)\n",
    "    sem_data_3 = np.std(data3, axis=1) / np.sqrt(iter_num)\n",
    "    sem_data_4 = np.std(data4, axis=1) / np.sqrt(iter_num)\n",
    "    sem_rdata_1 = np.std(rdata1, axis=1) / np.sqrt(iter_num)\n",
    "    sem_rdata_2 = np.std(rdata2, axis=1) / np.sqrt(iter_num)\n",
    "    sem_rdata_3 = np.std(rdata3, axis=1) / np.sqrt(iter_num)\n",
    "    sem_rdata_4 = np.std(rdata4, axis=1) / np.sqrt(iter_num)\n",
    "    \n",
    "    ci_lower_1 = np.mean(data1,axis=1) - z_value * sem_data_1\n",
    "    ci_upper_1 = np.mean(data1,axis=1) + z_value * sem_data_1\n",
    "    ci_lower_2 = np.mean(data2,axis=1) - z_value * sem_data_2\n",
    "    ci_upper_2 = np.mean(data2,axis=1) + z_value * sem_data_2\n",
    "    ci_lower_3 = np.mean(data3,axis=1) - z_value * sem_data_3\n",
    "    ci_upper_3 = np.mean(data3,axis=1) + z_value * sem_data_3\n",
    "    ci_lower_4 = np.mean(data4,axis=1) - z_value * sem_data_4\n",
    "    ci_upper_4 = np.mean(data4,axis=1) + z_value * sem_data_4\n",
    "\n",
    "    rci_lower_1 = np.mean(rdata1,axis=1) - z_value * sem_rdata_1\n",
    "    rci_upper_1 = np.mean(rdata1,axis=1) + z_value * sem_rdata_1\n",
    "    rci_lower_2 = np.mean(rdata2,axis=1) - z_value * sem_rdata_2\n",
    "    rci_upper_2 = np.mean(rdata2,axis=1) + z_value * sem_rdata_2\n",
    "    rci_lower_3 = np.mean(rdata3,axis=1) - z_value * sem_rdata_3\n",
    "    rci_upper_3 = np.mean(rdata3,axis=1) + z_value * sem_rdata_3\n",
    "    rci_lower_4 = np.mean(rdata4,axis=1) - z_value * sem_rdata_4\n",
    "    rci_upper_4 = np.mean(rdata4,axis=1) + z_value * sem_rdata_4\n",
    "    if mode == 'r':\n",
    "        max_point = max(np.max(np.array([np.mean(data1,axis=1),np.mean(data2,axis=1),np.mean(data3,axis=1),np.mean(data4,axis=1)]) ),0.6) + 0.1\n",
    "        max_point = 0.7\n",
    "    else :\n",
    "        max_point = max(np.max(np.array([np.mean(data1,axis=1),np.mean(data2,axis=1),np.mean(data3,axis=1),np.mean(data4,axis=1)]) ),0.4) + 0.1\n",
    "    for t in range(11):\n",
    "        if goodpval_list[0][t] == 1:\n",
    "            plt.plot(t,  max_point - 0.04 , marker='*', color='r', markersize=3)\n",
    "        if goodpval_list[1][t] == 1:\n",
    "            plt.plot(t,  max_point - 0.03, marker='*', color='g', markersize=3 ) \n",
    "        if goodpval_list[2][t] == 1:\n",
    "            plt.plot(t, max_point - 0.02, marker='*', color=cyan_c, markersize=3 ) \n",
    "        if goodpval_list[3][t] == 1:\n",
    "            plt.plot(t, max_point - 0.01, marker='*', color='magenta', markersize=4) \n",
    "    plt.fill_between(range(len(data1)), ci_lower_1, ci_upper_1, color='r', alpha=0.2)\n",
    "    plt.fill_between(range(len(data2)), ci_lower_2, ci_upper_2, color='g', alpha=0.2)\n",
    "    plt.fill_between(range(len(data3)), ci_lower_3, ci_upper_3, color=cyan_c, alpha=0.2)\n",
    "    plt.fill_between(range(len(data4)), ci_lower_4, ci_upper_4, color='magenta', alpha=0.2)\n",
    "    plt.fill_between(range(len(rdata1)), rci_lower_1, rci_upper_1, color='r', alpha=0.1)\n",
    "    plt.fill_between(range(len(rdata2)), rci_lower_2, rci_upper_2, color='g', alpha=0.1)\n",
    "    plt.fill_between(range(len(rdata3)), rci_lower_3, rci_upper_3, color=cyan_c, alpha=0.1)\n",
    "    plt.fill_between(range(len(rdata4)), rci_lower_4, rci_upper_4, color='magenta', alpha=0.1)\n",
    "    if mode == 'r' :\n",
    "        plt.ylim(-0.05,max_point)\n",
    "    if mode == 'r2' :\n",
    "        plt.ylim(-0.1,max_point)\n",
    "    \n",
    "    if titlename ==\"R2 score (train)\":\n",
    "        savepath=\"savedir/graph/\"+mode+\"_train_\"+pat_name +'_ch'+str(ch)+'_'+freq_str+'.png'\n",
    "    else:\n",
    "        savepath=\"savedir/graph/\"+mode+\"_\"+pat_name +'_ch'+str(ch)+'_'+freq_str+'.png'\n",
    "    plt.savefig(savepath,dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba863896-da3a-4613-960f-3f48e53e92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f21a2-73e7-4f53-90b5-f6518ab6f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_t_value(data1, data2):\n",
    "    t_stat, p_value = ttest_ind(data1, data2, equal_var=False)  \n",
    "    p_value_one_sided = p_value / 2 if t_stat > 0 else 1 - (p_value / 2)\n",
    "    return p_value_one_sided, t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5632b31-aa0e-4bf7-b28c-46de36093815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "ver=11\n",
    "iter_num = 1000\n",
    "np.random.seed(42) \n",
    "random_seeds = np.random.randint(0, 50000, size=iter_num)\n",
    "threshold = 0.05 / 11 / 624\n",
    "total_subjects=['sub1','sub2','sub3','sub4','sub5','sub6','sub7','sub8','sub9','sub10','sub11','sub12','sub13','sub14','sub15']\n",
    "freq_name = ['theta','alpha','beta','LG1','LG2','HG']\n",
    "for pat_name in total_subjects:\n",
    "    print(pat_name)\n",
    "    bad_trial_for_chs = bad_trial_giver(pat_name,strip_grid_no_list)\n",
    "    error_trial = liv_nonliv_badnull_ans_dict[pat_name][0]+liv_nonliv_badnull_ans_dict[pat_name][1]+body_nonbody_badnull_ans_dict[pat_name][0]+body_nonbody_badnull_ans_dict[pat_name][1]\n",
    "    l_data_Q, b_data_Q = file_loader_and_npconcate_question_TFmean(pat_name,q_offsettiming,liv_nonliv_raw_answer,body_nonbody_raw_answer,ver)\n",
    "    good_ch = getPatgoodch(pat_name, strip_grid_no_list)\n",
    "    raw_data_Q = np.concatenate((l_data_Q, b_data_Q))\n",
    "    raw_data_Q = raw_data_Q[:,:,good_ch,:]\n",
    "    pat_r_score_t_and_p = np.zeros((2,4,6,len(good_ch),11))\n",
    "    pat_r2_score_t_and_p_test = np.zeros((2,4,6,len(good_ch),11))\n",
    "    pat_r2_score_t_and_p_train = np.zeros((2,4,6,len(good_ch),11))\n",
    "    pat_r_score=np.zeros((2,4,6,len(good_ch),11,iter_num))\n",
    "    pat_r2_score_test=np.zeros((2,4,6,len(good_ch),11,iter_num)) #(random_ornot, 4type, 6freq, ch, time, iteration)\n",
    "    pat_r2_score_train=np.zeros((2,4,6,len(good_ch),11,iter_num)) \n",
    "    pat_r_coef = np.zeros((4,6,len(good_ch),11,iter_num,4))\n",
    "    freq = 5 #only HG\n",
    "    print(\"================\",freq_name[freq],\"================\")\n",
    "    for ch in range(len(good_ch)):\n",
    "            for t in range(11):\n",
    "                raw_data_Q_t = raw_data_Q[:, 100*t : 100*t +200 , : , :]\n",
    "                data_Q_t = np.mean(raw_data_Q_t,axis=1)\n",
    "                print(\"|\",end='')\n",
    "                for itr in range(iter_num): \n",
    "                    bad_trial = bad_trial_for_chs[0][ch]\n",
    "                    good_trial_Q = [i for i in range(144) if i not in bad_trial+error_trial]\n",
    "                    y_data_raw_Q = data_Q_t[:,ch,freq]\n",
    "                    #print(y_data_raw_Q.shape)\n",
    "                    y_data_Q = y_data_raw_Q[good_trial_Q]\n",
    "                    x_data_Q = total_answer[good_trial_Q]\n",
    "                    x_train, y_train, x_test_Q, y_test_Q = split_data(x_data_Q, y_data_Q, split_ratio=0.2, random_seed=random_seeds[itr])\n",
    "                    random.seed(int(random_seeds[itr]))\n",
    "                    shuffled_index = [i for i in range(len(x_train))]\n",
    "                    random.shuffle(shuffled_index) \n",
    "                    #print(x_train.shape, y_train.shape, x_test_Q.shape, y_test_Q.shape)\n",
    "\n",
    "                    \n",
    "                    model1 = LinearRegression()\n",
    "                    model1.fit(x_train[:,[0]],y_train)\n",
    "                    model1_predict = model1.predict(x_test_Q[:,[0]])\n",
    "                    model1_train_predict = model1.predict(x_train[:,[0]])\n",
    "                    \n",
    "                    model1_r = np.corrcoef(y_test_Q, model1_predict)[0, 1]\n",
    "                    model1_r2_test = r2_score(y_test_Q, model1_predict)\n",
    "                    model1_r2_train = r2_score(y_train, model1_train_predict)\n",
    "                    \n",
    "                    pat_r2_score_train[0,0,freq,ch,t,itr] =  model1_r2_train\n",
    "                    pat_r_score[0,0,freq,ch,t,itr] =  model1_r\n",
    "                    pat_r2_score_test[0,0,freq,ch,t,itr] =  model1_r2_test\n",
    "                    \n",
    "                    pat_r_coef[0,freq,ch,t,itr,0] = model1.coef_[0]\n",
    "                    pat_r_coef[0,freq,ch,t,itr,1] = model1.intercept_\n",
    "                    \n",
    "                    \n",
    "                    model2 = LinearRegression()\n",
    "                    model2.fit(x_train[:,[1]],y_train)\n",
    "                    model2_predict = model2.predict(x_test_Q[:,[1]])\n",
    "                    model2_train_predict = model2.predict(x_train[:,[1]])\n",
    "                    \n",
    "                    model2_r = np.corrcoef(y_test_Q, model2_predict)[0, 1]\n",
    "                    model2_r2_test = r2_score(y_test_Q, model2_predict)\n",
    "                    model2_r2_train = r2_score(y_train, model2_train_predict)\n",
    "                    \n",
    "                    pat_r2_score_train[0,1,freq,ch,t,itr] =  model2_r2_train\n",
    "                    pat_r_score[0,1,freq,ch,t,itr] =  model2_r\n",
    "                    pat_r2_score_test[0,1,freq,ch,t,itr] =  model2_r2_test\n",
    "                    \n",
    "                    pat_r_coef[1,freq,ch,t,itr,0] = model2.coef_[0]\n",
    "                    pat_r_coef[1,freq,ch,t,itr,1] = model2.intercept_\n",
    "                    \n",
    "                    \n",
    "                    model1_2 = LinearRegression()\n",
    "                    model1_2.fit(x_train[:,[0,1]],y_train)\n",
    "                    model1_2_predict = model1_2.predict(x_test_Q[:,[0,1]])\n",
    "                    model1_2_train_predict = model1_2.predict(x_train[:,[0,1]])\n",
    "                    \n",
    "                    model1_2_r = np.corrcoef(y_test_Q, model1_2_predict)[0, 1]\n",
    "                    model1_2_r2_test = r2_score(y_test_Q, model1_2_predict)\n",
    "                    model1_2_r2_train = r2_score(y_train, model1_2_train_predict)\n",
    "                    \n",
    "                    pat_r2_score_train[0,2,freq,ch,t,itr] =  model1_r2_train\n",
    "                    pat_r_score[0,2,freq,ch,t,itr] =  model1_2_r\n",
    "                    pat_r2_score_test[0,2,freq,ch,t,itr] =  model1_2_r2_test\n",
    "                    \n",
    "                    pat_r_coef[2,freq,ch,t,itr,[0,1]] = model1_2.coef_\n",
    "                    pat_r_coef[2,freq,ch,t,itr,2] = model1_2.intercept_\n",
    "\n",
    "                    \n",
    "                    model1_2_interaction= LinearRegression()\n",
    "                    model1_2_interaction.fit(x_train[:,[3,4,5]], y_train)\n",
    "                    model1_2_interaction_predict = model1_2_interaction.predict(x_test_Q[:,[3,4,5]])\n",
    "                    model1_2_interaction_train_predict = model1_2_interaction.predict(x_train[:,[3,4,5]])\n",
    "                    \n",
    "                    model1_2_interaction_r = np.corrcoef(y_test_Q, model1_2_interaction_predict)[0, 1]\n",
    "                    model1_2_interaction_r2_test = r2_score(y_test_Q, model1_2_interaction_predict)\n",
    "                    model1_2_interaction_r2_train = r2_score(y_train, model1_2_interaction_train_predict)\n",
    "                    \n",
    "                    pat_r_score[0,3,freq,ch,t,itr] =  model1_2_interaction_r\n",
    "                    pat_r2_score_test[0,3,freq,ch,t,itr] =  model1_2_interaction_r2_test\n",
    "                    pat_r2_score_train[0,3,freq,ch,t,itr] =  model1_2_interaction_r2_train\n",
    "                    \n",
    "                    pat_r_coef[3,freq,ch,t,itr,[0,1,2]] = model1_2_interaction.coef_\n",
    "                    pat_r_coef[3,freq,ch,t,itr,3] = model1_2_interaction.intercept_\n",
    "\n",
    "                    randmodel1 = LinearRegression()\n",
    "                    randmodel1.fit(x_train[shuffled_index,:][:,[0]],y_train)\n",
    "                    randmodel1_predict =randmodel1.predict(x_test_Q[:,[0]])\n",
    "                    randmodel1_train_predict = randmodel1.predict(x_train[shuffled_index,:][:,[0]])     \n",
    "                    randmodel1_r = np.corrcoef(y_test_Q, randmodel1_predict)[0, 1]\n",
    "                    randmodel1_r2_test = r2_score(y_test_Q, randmodel1_predict)\n",
    "                    randmodel1_r2_train = r2_score(y_train, randmodel1_train_predict)\n",
    "                    pat_r_score[1,0,freq,ch,t,itr] =  randmodel1_r\n",
    "                    pat_r2_score_test[1,0,freq,ch,t,itr] =  randmodel1_r2_test\n",
    "                    pat_r2_score_train[1,0,freq,ch,t,itr] =  randmodel1_r2_train\n",
    "\n",
    "\n",
    "                    randmodel2 = LinearRegression()\n",
    "                    randmodel2.fit(x_train[shuffled_index,:][:,[1]],y_train)\n",
    "                    randmodel2_predict =randmodel2.predict(x_test_Q[:,[1]])\n",
    "                    randmodel2_train_predict = randmodel2.predict(x_train[shuffled_index,:][:,[1]])\n",
    "                    randmodel2_r = np.corrcoef(y_test_Q, randmodel2_predict)[0, 1]\n",
    "                    randmodel2_r2_test = r2_score(y_test_Q, randmodel2_predict)\n",
    "                    randmodel2_r2_train = r2_score(y_train, randmodel2_train_predict)\n",
    "                    pat_r_score[1,1,freq,ch,t,itr] =  randmodel2_r\n",
    "                    pat_r2_score_test[1,1,freq,ch,t,itr] =  randmodel2_r2_test\n",
    "                    pat_r2_score_train[1,1,freq,ch,t,itr] =  randmodel2_r2_train\n",
    "                    \n",
    "\n",
    "                    randmodel1_2 = LinearRegression()\n",
    "                    randmodel1_2.fit(x_train[shuffled_index,:][:,[0,1]],y_train)\n",
    "                    randmodel1_2_predict =randmodel1_2.predict(x_test_Q[:,[0,1]])\n",
    "                    randmodel1_2_train_predict = randmodel1_2.predict(x_train[shuffled_index,:][:,[0,1]])\n",
    "                    randmodel1_2_r = np.corrcoef(y_test_Q, randmodel1_2_predict)[0, 1]\n",
    "                    randmodel1_2_r2_test = r2_score(y_test_Q, randmodel1_2_predict)\n",
    "                    randmodel1_2_r2_train = r2_score(y_train, randmodel1_2_train_predict)\n",
    "                    pat_r_score[1,2,freq,ch,t,itr] =  randmodel1_2_r\n",
    "                    pat_r2_score_test[1,2,freq,ch,t,itr] =  randmodel1_2_r2_test\n",
    "                    pat_r2_score_train[1,2,freq,ch,t,itr] =  randmodel1_2_r2_train\n",
    "\n",
    "                    randmodel1_2_interaction = LinearRegression()\n",
    "                    randmodel1_2_interaction.fit(x_train[shuffled_index,:][:,[3,4,5]],y_train)\n",
    "                    randmodel1_2_interaction_predict =randmodel1_2_interaction.predict(x_test_Q[:,[3,4,5]])\n",
    "                    randmodel1_2_interaction_train_predict = randmodel1_2_interaction.predict(x_train[shuffled_index,:][:,[3,4,5]])\n",
    "                    randmodel1_2_interaction_r = np.corrcoef(y_test_Q, randmodel1_2_interaction_predict)[0, 1]\n",
    "                    randmodel1_2_interaction_r2_test = r2_score(y_test_Q, randmodel1_2_interaction_predict)\n",
    "                    randmodel1_2_interaction_r2_train = r2_score(y_train, randmodel1_2_interaction_train_predict)\n",
    "                    pat_r_score[1,3,freq,ch,t,itr] =  randmodel1_2_interaction_r\n",
    "                    pat_r2_score_test[1,3,freq,ch,t,itr] =  randmodel1_2_interaction_r2_test\n",
    "                    pat_r2_score_train[1,3,freq,ch,t,itr] =  randmodel1_2_interaction_r2_train\n",
    "                    \n",
    "                #pvalue \n",
    "                r_model1_p, r_model1_t = p_t_value(pat_r_score[0,0,freq,ch,t,:], pat_r_score[1,0,freq,ch,t,:])\n",
    "                r_model2_p, r_model2_t = p_t_value(pat_r_score[0,1,freq,ch,t,:], pat_r_score[1,1,freq,ch,t,:])\n",
    "                r_model3_p, r_model3_t = p_t_value(pat_r_score[0,2,freq,ch,t,:], pat_r_score[1,2,freq,ch,t,:])\n",
    "                r_model4_p, r_model4_t = p_t_value(pat_r_score[0,3,freq,ch,t,:], pat_r_score[1,3,freq,ch,t,:])\n",
    "                pat_r_score_t_and_p[0,0,freq,ch,t] = r_model1_t\n",
    "                pat_r_score_t_and_p[1,0,freq,ch,t] = r_model1_p\n",
    "                pat_r_score_t_and_p[0,1,freq,ch,t] = r_model2_t\n",
    "                pat_r_score_t_and_p[1,1,freq,ch,t] = r_model2_p\n",
    "                pat_r_score_t_and_p[0,2,freq,ch,t] = r_model3_t\n",
    "                pat_r_score_t_and_p[1,2,freq,ch,t] = r_model3_p\n",
    "                pat_r_score_t_and_p[0,3,freq,ch,t] = r_model4_t\n",
    "                pat_r_score_t_and_p[1,3,freq,ch,t] = r_model4_p\n",
    "                r2_model1_p, r2_model1_t = p_t_value(pat_r2_score_test[0,0,freq,ch,t,:], pat_r2_score_test[1,0,freq,ch,t,:])\n",
    "                r2_model2_p, r2_model2_t = p_t_value(pat_r2_score_test[0,1,freq,ch,t,:], pat_r2_score_test[1,1,freq,ch,t,:])\n",
    "                r2_model3_p, r2_model3_t = p_t_value(pat_r2_score_test[0,2,freq,ch,t,:], pat_r2_score_test[1,2,freq,ch,t,:])\n",
    "                r2_model4_p, r2_model4_t = p_t_value(pat_r2_score_test[0,3,freq,ch,t,:], pat_r2_score_test[1,3,freq,ch,t,:])\n",
    "                pat_r2_score_t_and_p_test[0,0,freq,ch,t] = r2_model1_t\n",
    "                pat_r2_score_t_and_p_test[1,0,freq,ch,t] = r2_model1_p\n",
    "                pat_r2_score_t_and_p_test[0,1,freq,ch,t] = r2_model2_t\n",
    "                pat_r2_score_t_and_p_test[1,1,freq,ch,t] = r2_model2_p\n",
    "                pat_r2_score_t_and_p_test[0,2,freq,ch,t] = r2_model3_t\n",
    "                pat_r2_score_t_and_p_test[1,2,freq,ch,t] = r2_model3_p\n",
    "                pat_r2_score_t_and_p_test[0,3,freq,ch,t] = r2_model4_t\n",
    "                pat_r2_score_t_and_p_test[1,3,freq,ch,t] = r2_model4_p\n",
    "                r2_model1_p, r2_model1_t = p_t_value(pat_r2_score_train[0,0,freq,ch,t,:], pat_r2_score_train[1,0,freq,ch,t,:])\n",
    "                r2_model2_p, r2_model2_t = p_t_value(pat_r2_score_train[0,1,freq,ch,t,:], pat_r2_score_train[1,1,freq,ch,t,:])\n",
    "                r2_model3_p, r2_model3_t = p_t_value(pat_r2_score_train[0,2,freq,ch,t,:], pat_r2_score_train[1,2,freq,ch,t,:])\n",
    "                r2_model4_p, r2_model4_t = p_t_value(pat_r2_score_train[0,3,freq,ch,t,:], pat_r2_score_train[1,3,freq,ch,t,:])\n",
    "                pat_r2_score_t_and_p_train[0,0,freq,ch,t] = r2_model1_t\n",
    "                pat_r2_score_t_and_p_train[1,0,freq,ch,t] = r2_model1_p\n",
    "                pat_r2_score_t_and_p_train[0,1,freq,ch,t] = r2_model2_t\n",
    "                pat_r2_score_t_and_p_train[1,1,freq,ch,t] = r2_model2_p\n",
    "                pat_r2_score_t_and_p_train[0,2,freq,ch,t] = r2_model3_t\n",
    "                pat_r2_score_t_and_p_train[1,2,freq,ch,t] = r2_model3_p\n",
    "                pat_r2_score_t_and_p_train[0,3,freq,ch,t] = r2_model4_t\n",
    "                pat_r2_score_t_and_p_train[1,3,freq,ch,t] = r2_model4_p\n",
    "            \n",
    "            r_good_pval_point = [np.where(pat_r_score_t_and_p[1,0,freq,ch,:]<threshold,1,0), np.where(pat_r_score_t_and_p[1,1,freq,ch,:]<threshold,1,0),np.where(pat_r_score_t_and_p[1,2,freq,ch,:]<threshold,1,0),np.where(pat_r_score_t_and_p[1,3,freq,ch,:]<threshold,1,0)]\n",
    "            r2_good_pval_point_test = [np.where(pat_r2_score_t_and_p_test[1,0,freq,ch,:]<threshold,1,0), np.where(pat_r2_score_t_and_p_test[1,1,freq,ch,:]<threshold,1,0),np.where(pat_r2_score_t_and_p_test[1,2,freq,ch,:]<threshold,1,0),np.where(pat_r2_score_t_and_p_test[1,3,freq,ch,:]<threshold,1,0)]\n",
    "            r2_good_pval_point_train = [np.where(pat_r2_score_t_and_p_train[1,0,freq,ch,:]<threshold,1,0), np.where(pat_r2_score_t_and_p_train[1,1,freq,ch,:]<threshold,1,0),np.where(pat_r2_score_t_and_p_train[1,2,freq,ch,:]<threshold,1,0),np.where(pat_r2_score_t_and_p_train[1,3,freq,ch,:]<threshold,1,0)]          \n",
    "            four_paired_data_plotter(pat_r_score[0,0,freq,ch,:,:],pat_r_score[0,1,freq,ch,:,:],pat_r_score[0,2,freq,ch,:,:],pat_r_score[0,3,freq,ch,:,:],                    \n",
    "                                     pat_r_score[1,0,freq,ch,:,:],pat_r_score[1,1,freq,ch,:,:],pat_r_score[1,2,freq,ch,:,:],pat_r_score[1,3,freq,ch,:,:],\n",
    "                                     iter_num, pat_name, freq_name[freq],\"Corr. (r)\", ch,r_good_pval_point,mode=\"r\")\n",
    "            four_paired_data_plotter(pat_r2_score_test[0,0,freq,ch,:,:],pat_r2_score_test[0,1,freq,ch,:,:],pat_r2_score_test[0,2,freq,ch,:,:],pat_r2_score_test[0,3,freq,ch,:,:],\n",
    "                                     pat_r2_score_test[1,0,freq,ch,:,:],pat_r2_score_test[1,1,freq,ch,:,:],pat_r2_score_test[1,2,freq,ch,:,:],pat_r2_score_test[1,3,freq,ch,:,:],\n",
    "                                     iter_num, pat_name, freq_name[freq],\"R2 score\", ch, r2_good_pval_point_test, mode=\"r2\")\n",
    "            four_paired_data_plotter(pat_r2_score_train[0,0,freq,ch,:,:],pat_r2_score_train[0,1,freq,ch,:,:],pat_r2_score_train[0,2,freq,ch,:,:],pat_r2_score_train[0,3,freq,ch,:,:],\n",
    "                                     pat_r2_score_train[1,0,freq,ch,:,:],pat_r2_score_train[1,1,freq,ch,:,:],pat_r2_score_train[1,2,freq,ch,:,:],pat_r2_score_train[1,3,freq,ch,:,:],\n",
    "                                     iter_num, pat_name, freq_name[freq],\"R2 score (train)\", ch, r2_good_pval_point_train, mode=\"r2\")\n",
    "    savepath = \"save_dir/\"+pat_name+\"_r2score_test.pkl\"\n",
    "    with open(savepath, 'wb') as f:\n",
    "        pickle.dump(pat_r2_score_test, f, pickle.HIGHEST_PROTOCOL)\n",
    "    savepath = \"save_dir/\"+pat_name+\"_r2score_train.pkl\"\n",
    "    with open(savepath, 'wb') as f:\n",
    "        pickle.dump(pat_r2_score_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "    savepath = \"save_dir/\"+pat_name+\"_corr_r.pkl\"\n",
    "    with open(savepath, 'wb') as f:\n",
    "        pickle.dump(pat_r_score, f, pickle.HIGHEST_PROTOCOL)             \n",
    "    savepath = \"save_dir/\"+pat_name+\"_r_coef.pkl\"\n",
    "    with open(savepath, 'wb') as f:\n",
    "        pickle.dump(pat_r_coef, f, pickle.HIGHEST_PROTOCOL)\n",
    "    savepath = \"save_dir/\"+pat_name+\"_p_and_t.pkl\"\n",
    "    with open(savepath, 'wb') as f:\n",
    "        pickle.dump([pat_r_score_t_and_p,pat_r2_score_t_and_p_test,pat_r2_score_t_and_p_train ], f, pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "\n",
    "\n",
    "                    \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f9bb1-5c51-4b2d-bc42-4a179dbe38d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
